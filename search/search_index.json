{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"JET Encoding Guide","text":"<p>Welcome to the JET Encoding Guide.</p>"},{"location":"#about-this-guide","title":"About this Guide","text":"<p>Currently this \"guide\" is just a loose collection of pages with useful information related to multimedia, encoding, and VapourSynth.</p> <p>The idea was to just get started with collecting information online as quickly as possible for now, instead of spending months and months debating the perfect structure for a full guide and never getting anywhere. Properly structuring this guide and filling in gaps can then be done in the future. Of course, help is always welcome.</p> <p>The guide started out with the Encoding Resources page. While this guide is still under construction, that page is where you find resources covering anything not yet covered here.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>In line with the philosophy outlined above, any and all contributions are welcome and should be merged fairly quickly as long as they are factually correct and remotely intelligible. The goal is to make the barrier to entry for contributors as low as possible and prevent patches from being stuck in \"review hell\" for months just because they're not structured or worded perfectly.</p>"},{"location":"resources/","title":"List of Encoding Resources","text":"<p>This page was originally the Encoding Resources gist on my (arch1t3cht's) GitHub. At the moment, this page is just a full copy-paste of that guide. In the future, when this guide website grows and more topics get their own pages, some of this page's content can be moved there.</p> <p>So this is supposed to be a list of encoding-related resources together with some very basic instructions. Kind of an encoding analogue to fansub.html. This is not a full guide on encoding.</p> <p>Since this page is starting to get linked elsewhere, I should also make clear that it mostly comes from (a person adjacent to) the JET community. In particular, it's written primarily from the perspective of anime encoding. Still, most parts will hold up equally well for live action and other areas.</p> <p>This guide may seem fairly technical. Partly this is because I have a background in pure mathematics and this is how I learned the material, but partly it's just because encoding is cursed and complicated, and you will need to learn the things listed here in order to not make stupid mistakes.</p> <p>Some sections contain some longer rants about stuff, most others don't. This is almost entirely based on the mood I was in when writing them.</p> <p>I also give no guarantees of accuracy for any of the linked sites. I left out most links that are outdated or plain wrong, but I also did not audit every single page in detail. This is mostly just my bookmarks folder written up in markdown.</p>"},{"location":"resources/#basics","title":"Basics","text":"<ul> <li>Most video files you'll come across are stored in a YCbCr colorspace with 4:2:0 subsampled chroma and limited color range.</li> <li>Wikipedia article on YCbCr (More info is in the colorspaces section further below)</li> <li>Wikipedia article on chroma subsampling</li> </ul>"},{"location":"resources/#math","title":"Math","text":"<p>Encoding is hard and technical. If you want to really understand what you're doing, you'll need a bit of math background. Some relevant topics are:</p> <ul> <li>Some basic linear algebra</li> <li>Knowing what a convolution is and how they work (3b1b video)</li> <li>Understanding Fourier transforms and, more generally, the concept of thinking in frequency space (Imagemagick docs on Fourier transforms)</li> </ul>"},{"location":"resources/#how-to-learn","title":"How to Learn","text":"<p>Whatever you're trying to learn (this is not even specific to encoding), it cannot be stressed enough that you should try things out and use your resources. That's the entire reason why this document exists. Sure, there might be people to ask for help on simpler topics, but some day you'll reach a point where there's no one who can answer your question and you'll need to figure it out yourself. And even before that, people will be much more willing to help you if you've shown some effort of solving your problem yourself.</p>"},{"location":"resources/#colorspaces","title":"Colorspaces","text":"<ul> <li>For the theory, read the Wikipedia article on the CIE RGB Colorspace</li> <li>Zoomers that don't want to read can watch this video as an introduction (or at least the part that explains color spaces)</li> <li>For RGB-based colorspaces, the conversion chain is: CIE RGB -&gt; RGB (using primaries and white point), RGB -&gt; R'G'B' (using transfer function), R'G'B' -&gt; Y'CbCr (using color matrix)</li> <li>For specific examples, read the articles on BT.709 or BT.601, or the actual standards (BT.709, BT.601)</li> <li>Doom9 summary thread about colorspaces</li> <li>Poynton's Gamma and Colour FAQs</li> <li>The color-related articles on ImageWorsener's page</li> <li>List of primaries/transfer functions/matrices used in VapourSynth The tables there being taken from the HEVC specification (Appendix E)</li> <li>fmtconv docs also have a large list of color spaces and their relationships</li> <li>The QuickTime Gamma Bug</li> </ul>"},{"location":"resources/#hdr","title":"HDR","text":"<p>The terminology related to HDR is a huge mess, but conceptually HDR consists of two aspects</p> <ul> <li>New transfer functions (HLG and PQ) that allow for a much higher dynamic range in colors</li> <li>Formats for metadata that helps players and screens convert their HDR inputs to colors that they can display</li> </ul> <p>While HDR primarily only concerns a different transfer function, it is often paired with a switch to BT.2020 primaries, even if their full gamut is not actually used (many HDR videos use the P3 gamut, for example).</p> <p>A few resources:</p> <ul> <li>BT.2100, SMPTE ST 2084, SMPTE ST 2086, BT2446</li> <li>Sheet on Dolby Vision Stuff</li> <li>dovi_tool</li> <li>vs-nlq: VapourSynth plugin to map DV enhancement layers. The BL needs to be 16 bit, the EL needs to be 10 bit and point-upscaled to the BL's size.</li> </ul>"},{"location":"resources/#vapoursynth","title":"VapourSynth","text":"<p>Understanding how exactly the VapourSynth ecosystem works and which parts play what roles is crucial when working with it. Otherwise, you will not be able to find the documentation you need or pinpoint where exactly your errors are coming from.</p> <p>VapourSynth (at least the parts relevant for us) itself can be seen as consisting of three components:</p> <ul> <li>The core of VapourSynth is a frame server. It's able to chain together various functions, where each function (well, most functions) can take a sequence of video frames (or multiple sequences) together with some parameters, modify those frames in some way, and output the resulting sequence of frames (or other values).   Such sequences of frames are called video nodes, and they are computed lazily: Frames are only computed when requested, and they are only requested when they're required in other functions. This allows VapourSynth to process a video frame by frame without having to store the entire clip in memory.   Video nodes also contain frame props, which are pieces of data (key-value pairs) associated with each frame that functions can use and change as they please.</li> </ul> <p>VapourSynth offers a C/C++ api to call functions on video nodes and can load third-party plugins which provide functions.   It supports common video pixel formats, but apart from that the core of VapourSynth knows next to nothing about how a video actually looks. - On top of this architecture, VapourSynth then provides a small set of standard functions to perform various simple operations on video nodes.   Some simple examples are concatenating clips, selecting or deleting frames from clips, flipping or cropping the picture, or modifying frame props.   Some of the more sophisticated functions are the Convolution and Expr functions and the resize family of functions which can resample and convert between pixel formats.</p> <p>Furthermore, VapourSynth defines a set of reserved frame props to denote common video properties like the frame rate, sample aspect ratio, scan type, or color space. - Finally, VapourSynth implements Python bindings for its filtering architecture and provides a VSScript API which allows one to run a Python script to create, process, and output video nodes.   With these Python bindings, functions on video nodes can be called as simple member functions on the node objects, and common operations like joining or slicing can be performed using common Python operators.   Furthermore, VapourSynth provides a simple program called <code>vspipe</code> that uses the VSScript API to execute such a Python script and output the frames it generates to then be passed to an encoder.   This then results in the VapourSynth script workflow you probably know.</p> <p>Then, there are three further components in the wider VapourSynth ecosystem:</p> <ul> <li>There are dozens of VapourSynth plugins written by users (or sometimes by the authors of VapourSynth themselves) which provide all kinds of functions to process video nodes.   These are the filtering plugins that make up the real heart of VapourSynth</li> <li>There are various programs using the VapourSynth or VSScript APIs in order to somehow apply a filterchain or script to some video clip.   These range from VapourSynth script editor/preview programs like VSEdit and vs-preview and utility programs for filtering like Wobbly to programs which just use VapourSynth as a means to an end to load or process video, like mpv or (forks of) Aegisub.</li> <li>There are Python libraries that provide wrappers for various existing plugins in order to cut down on boilerplate and make them easier to use with a Pythonic mindset. This is primarily the family of JET packages.</li> </ul> <p>I'm placing a lot of emphasis on this distinction for two reasons: On one hand, understanding it can resolve many misconceptions about the ecosystem. VapourSynth isn't written in Python, it's written in C++ and provides Python bindings. It's completely possible to create API bindings for other languages (and has been done for some). VapourSynth's library of standard functions is extremely useful, but in theory it would be possible to set up an entire filterchain without using a single standard function. Python wrappers for filters are different from the filters themselves, and the two are installed and used in entirely different ways.</p> <p>On the other hand, understanding the role of each piece in the machine will help you know where you need to go to find the information you need: Want to know how to use <code>vsdenoise.BM3DCuda</code>? Well, you can check the docstrings and ask your Python language server to find out how to call it, but in the end this class is just a wrapper for the <code>bm3dcuda</code> plugin, so if you want to know what the parameters do you should read the documentation of that plugin. And if you want to know what BM3D actually does, you should read the paper the plugin is based on. But then, if you're wondering what the <code>matrix</code> argument for <code>vsdenoise</code>'s <code>BM3DCuda</code> is for, well, that refers to a color matrix and you should read the docs of the VapourSynth standard library for that (as well as those of <code>vstools</code> for a more convenient wrapper, as your language server might tell you).</p> <p>Now, after writing all of this, I should point out that this is not a perfect distinction, since for example it is also possible to implement filters directly in Python with certain more advanced techniques. Still, the point of explaining the VapourSynth ecosystem this way is not to give a perfect and complete description of it, but to help beginners understand how the various components interact and where various errors come from.</p> <p>With this in mind, here are a couple of general resources on VapourSynth:</p> <ul> <li>Full VapourSynth Docs</li> <li>List of Reserved FrameProps</li> <li>VapourSynth Standard Functions</li> <li>Resize Docs Also contains tables explaining the various matrix/primaries/transfer values</li> <li>Learn some basic Python. Use any resource you want, like this one. You need to know basic Python if you want to write VapourSynth scripts, and people will not be very patient with you if half of your questions on VapourSynth just come from a lack of Python knowledge.</li> </ul> <p>As for the various filter plugins and wrappers, there's too many of those to list here so just check their documentation.</p> <p>Again, remember that Python wrappers really are just wrappers. They're very helpful if you want to write VapourSynth scripts without too much boilerplate, but for learning how certain filters work it can be very helpful to play around with the raw plugins a bit. Understanding how the plugins work will help you understand what the wrappers do.</p>"},{"location":"resources/#filtering","title":"Filtering","text":"<p>This is a huge umbrella topic and the general advice still remains \"Find out what filters exist for a given use case and try them out.\"</p> <p>Keep in mind that there is no magical way to recover information, so any filter will be destructive to some degree. Don't use a filter if your source does not have the problem the filter is supposed to fix, or if the filter causes more issues than it fixes. Use your eyes to check for issues and do not blindly rely on automated metrics.</p> <p>Recognizing artifacts:</p> <ul> <li>guide.encode.moe's page on artifacts</li> <li>bakashots.me reference list for artifacts</li> </ul> <p>Unfortunately, neither of these is complete.</p> <p>After giving these lists of artifacts it should be stressed again that you should not try to fix an artifact that isn't there. Your encoding process should be \"See what artifacts the source has, then try to fix them,\" not \"Ok, so my script should always have denoising, dehaloing, rescaling, antialiasing, debanding, and regraining.\" This is also the case when you cannot see an artifact that a source is supposed to have, even when others tell you it's there. (Though of course this means that you should try to find out what's going on and learn to spot this artifact.) If you can't see that an artifact is there, you also won't be able to judge whether your filtering fixes it.</p> <p>Finally, if your source doesn't have any significant artifacts, that doesn't mean that you should throw filters at it to somehow still improve how it looks. It just means that maybe you don't even need to encode it.</p>"},{"location":"resources/#resampling","title":"Resampling","text":"<ul> <li>Avisynth docs on resampling</li> <li>Imagemagick docs on resampling</li> <li>guide.encode.moe's page on resampling</li> <li>Docs for VapourSynth's Resize This explains the meaning of parameters like <code>src_width</code> and <code>src_top</code>.</li> <li>The resampling-related articles on ImageWorsener's page and ResampleScope's page</li> <li>Thingo I made with diagrams for all of this</li> </ul> <p>It is extremely important to realize that upsampling and downsampling are two fundamentally different operations. A kernel that's good for upsampling does not need to be good for downsampling and vice-versa.</p> <p>Conventional resampling (no matter if upsampling or downsampling) is linear (except for value clipping or when implicitly padding with a non-zero brightness value). This means that any horizontal resampling operation will commute with any vertical resampling operation and vice-versa.</p>"},{"location":"resources/#upsampling","title":"Upsampling","text":"<p>Conceptually, upsampling is divided into two steps</p> <ul> <li>Reconstruction: Convolve with the resampling kernel to obtain a continuous<sup>1</sup> function. This step only depends on the kernel used</li> <li>Sampling: Sample the reconstructed function with a different sampling grid, determined by <code>width</code>/<code>height</code>, <code>src_width</code>/<code>src_height</code>, and <code>src_left</code>/<code>src_top</code>.</li> </ul> <p>Different kernels will yield different results with different artifacts. Traditional convolution-based resampling will always be a trade-off between blurring, aliasing, and ringing/haloing.</p> <p>Note that aliasing is often conflated with blocking, but technically those are two different notions: Aliasing is about low frequencies incorrectly being reconstructed to high-frequencies, while blocking (more formally referred to as anisotropy) is specifically an effect of tensor resampling (and can thus only occur in 2D or higher dimensions) and is caused by the (2D) resampling kernel not being radially symmetric. Blocking can be partially alleviated by using a polar (or EWA) kernel, while aliasing cannot.</p> <p>Here's some more resources on upsampling in particular</p> <ul> <li>For more mathematical background, read the paper by Mitchell-Netravali (and if you want to dive even deeper, read some of the papers that references, like [KEY81] and [PAR83])</li> <li>Desmos graph visualizing the upsampling process</li> <li>Plots of some common resampling kernels Note that the Blurring/Sharpness/Ringing graphs on the bottom aren't really reliable.</li> </ul>"},{"location":"resources/#downsampling","title":"Downsampling","text":"<p>Downsampling is an entirely different process from upsampling. Applying the process used for upsampling to downsampling will result in massive aliasing no matter what reconstruction kernel is used. Thus, instead of asking how to best reconstruct a continuous function out of the samples like with upsampling, the main question when downsampling is how to prevent aliasing. This is done by applying a lowpass filter to reduce the high frequences that would cause aliasing. This is also indirectly covered in the Mitchell-Netravali paper.</p> <p>With this in mind, good downsampling kernels are kernels that result in good lowpass filters like Gaussian kernels, or faster approximations to them like Hermite. In situations where you're worried about Moir\u00e9 patterns, Mitchell is also a good candidate. But as a rule of thumb, kernels with strong negative lobes will not make good downsampling kernels, even if they're fantastic upsampling kernels.</p>"},{"location":"resources/#descaling","title":"Descaling","text":"<p>The goal of a descale is to mathematically invert an upscale. Never descale a video unless you're absolutely sure that it was upscaled with those exact parameters, and that no additional post-processing was done afterwards.</p> <p>Once you know what parameters your clip was upscaled with, the signature of the descale function should tell you everything you need to call the plugin. A descale call with given <code>kernel</code> and <code>src_width</code>, <code>src_height</code>, <code>src_left</code>, <code>src_top</code> parameters will invert a <code>core.resize</code> call with the exact same values. With a fractional descale, the parity of the width/height you're descaling to makes an important difference (and changing the parity amounts to a shift by <code>0.5</code>), but apart from the parity the width/height does not matter.</p> <p>For evaluating whether your descale parameters are correct, you should check both the descale and the rescale (i.e. the upscale of the descale with the same parameters you descaled with). If the rescale's lineart looks different from the original clip, the descale cannot have been accurate. But for sharp kernels, the rescale can be very close to the original clip even for incorrect descales, so you need to check the descale too. If the descale has higher-order haloing (and usually even if it has first-order haloing<sup>2</sup>), it's not going to be correct.</p> <p>Rescale error is a decent metric to get estimates for a source resolution or shifts, but it's never the full story. Do not pick kernels based on lowest rescale error.</p> <p>Dirty borders (when they exist) can be another indicator as to whether a descale is correct, but it seems like not all dirty borders are fixed by descales. We don't really know enough about the causes of dirty borders yet to be more certain here.</p> <p>Like (tensor) resampling, descaling is done in separate steps per axis. Furthermore, the operation is linear, so (again, except for clipping) it will commute with any resampling or descale operation along the other axis. For finding descale parameters, it can be useful to analyze the horizontal and vertical axes separately, though this can make it more difficult to visually identify correct descales. Some footage can only be descaled along one axis.</p> <p>Do not descale subsampled chroma. This should be clear from the previous points but experience shows that it needs to be spelled out explicitly. Similarly, do not (horizontally) descale footage that went through the HDCAM format (and same for any other formats with subsampled luma).</p>"},{"location":"resources/#formats-and-encoders","title":"Formats and Encoders","text":"<ul> <li>Overview of the High Efficiency Video Coding (HEVC) Standard for a brief overview of how modern coding formats work</li> <li>For more detailed information, pick up a textbook like High Efficiency Video Coding - Coding Tools and Specification</li> <li>The actual standards (H.264 and H.265) probably won't help you unless you have extremely specific questions</li> </ul>"},{"location":"resources/#x264","title":"x264","text":"<ul> <li>Overview of x264's rate control modes (without <code>mbtree</code>)</li> <li>MeGUI's x264 settings list</li> <li>MeWiki's x264 settings list</li> <li>Use the silentaperture guide for decent starter settings</li> </ul>"},{"location":"resources/#x265","title":"x265","text":"<ul> <li>x265 is based on x264 so many of the general concepts can be carried over</li> <li>x265 docs</li> <li>Use the silentaperture guide for decent starter settings</li> </ul>"},{"location":"resources/#ivtc","title":"IVTC","text":"<p>IVTC is completely different from deinterlacing. NEVER try to \"IVTC\" by calling QTGMC or anything like that. Also, never use AnimeIVTC.</p> <ul> <li>Understanding 3:2 Pulldown: Wikipedia Page, Wobbly Guide's Page on Telecining</li> <li>fieldbased.media</li> <li> <p>The basic concept of IVTC:</p> <p>Conceptually, IVTC is split into two steps, called fieldmatching and decimation. (Sometimes, it also needs additional post-processing steps like interpolating orphans, freezeframing, fixing fades, etc.) Fieldmatching rearranges the video's fields to try and match every field with its original counterpart. This results in a clip that ideally no longer has any combing (in practice this may not be the case due to complications like orphans, fades, etc), but will still be 30fps since it still contains duplicate frames. The decimation step then drops those duplicate frames to obtain a 24p clip.</p> <p>The Decomb docs (here and here) also illustrate this process pretty well.</p> </li> <li> <p>Understanding fieldmatching: Read the Background and Overview section of the TIVTC docs</p> </li> <li>There exist automated methods for IVTC (TIVTC, VIVTC, but note that TDecimate for VapourSynth is broken), but if you want good results you'll need to manually IVTC with a tool like Wobbly.</li> <li>Wobbly Guide</li> <li>ivtc.org (archived)</li> <li>The Yatta Manifesto</li> </ul>"},{"location":"resources/#other-sd-era-sadness-ntscpal-dvds-and-all-that","title":"Other SD Era Sadness (NTSC/PAL, DVDs, and all that)","text":"<ul> <li>Lurker's Guide Collection of various guides on video. Includes guides like \"Programmer's Guide to Video Systems\" and \"All about Video Fields\"</li> <li>SMPTE RP 187-1995</li> <li>A Quick Guide to Digital Video Resolution and Aspect Ratio Conversions Reference for DVD aspect ratios</li> <li>The 625/50 PAL Video Signal and TV Compatible Graphics Modes Background for the previous guide, explaining the analog PAL signal in detail</li> <li>Google Sheet on Analog Video Resolutions</li> <li>DVD-Video Information</li> <li>The DVD FAQ</li> </ul>"},{"location":"resources/#miscellaneous-stuff-mostly-blogs","title":"Miscellaneous Stuff (mostly blogs)","text":"<ul> <li>torchlight's blog</li> <li>Diary Of An x264 Developer</li> <li>Falsehoods programmers believe about [video stuff]</li> <li>Sneedex Site listing the best encodes and/or sources for many anime shows, with comparisons</li> <li>JET Discord Server</li> </ul> <ol> <li> <p>Here, continuous means \"defined on a continuous domain\", as opposed to the discrete list of samples that was given as an input. The reconstructed function does not necessarily have to be continuous in the \"no sudden jumps in values\" sense.\u00a0\u21a9</p> </li> <li> <p>A descale having first-order haloing is theoretically possible if you believe that the image was sharpened before upscaling, but this is very unlikely in practice. In the vast majority of cases, haloing in a descale means that the descale is incorrect.\u00a0\u21a9</p> </li> </ol>"},{"location":"basics/howdoi/","title":"How do I ...?","text":"<p>This page collects examples on how to perform simple tasks in VapourSynth. It's mostly about how to shuffle clips around and how to convert between formats, not about actual filtering.</p> <p>Many of the things here can also be found on their respective documentation pages (e.g. VapourSynth's Python Reference and Function Reference), so go there if you need more details on any function. The point of this page is to make the barrier to entry lower.</p> <p>Some of the entries here list more than one way to achieve a certain goal. For example, they may show both a way using only standard VapourSynth functions, and a way using JET wrappers. Apart from just providing multiple options, this is also done to show that many of the wrappers around simple operations are not magic, and really just call standard functions under the hood. In the end, which method you use is up to you. Unless otherwise stated, there isn't any relevant difference between them, except for one option being easier to write than the other.</p>"},{"location":"basics/howdoi/#how-do-i-cut-off-frames-at-the-beginningend-of-a-clip","title":"How do I cut off frames at the beginning/end of a clip?","text":"Python SyntaxTrim filter <p>Clips can be cut by simply slicing them like Python lists:</p> <pre><code>clip_cut = clip[10:]    # Cut off 10 frames at the start\nclip_cut = clip[:-10]   # Cut off 10 frames at the end\n\n# Start at frame 10 of the clip, and go up until frame 999.\n# Frame 1000 is not included.\nclip_cut = clip[10:1000]\n</code></pre> <p>Just like everything else, cutting clips can also be done via a filter invocation. There's no real use for this (unless you're doing fancy things like passing around filter functions as objects, in which case you probably don't need to read this page), except for knowing that slicing is not magic.</p> <p>Note that the Trim filter, unlike slicing, is inclusive.</p> <pre><code>clip_cut = clip.std.Trim(first=10)    # Cut off 10 frames at the start\nclip_cut = clip.std.Trim(length=clip.num_frames - 10)   # Cut off 10 frames at the end\n\n# Start at frame 10 of the clip, and go up until frame 1000,\n# which is included.\nclip_cut = clip.std.Trim(first=10, last=1000)\n</code></pre>"},{"location":"basics/howdoi/#how-do-i-cut-out-a-section-of-frames-from-a-clip","title":"How do I cut out a section of frames from a clip?","text":"<p>See above.</p>"},{"location":"basics/howdoi/#how-do-i-join-multiple-clips-together","title":"How do I join multiple clips together?","text":"<p>Clips can be joined by simply using the <code>+</code> operator in Python: <pre><code>clip_joined = clip1 + clip2 + clip3\n</code></pre> If you need to join many clips together, or have a list of clips, it may be simpler to use <code>core.std.Splice</code>: <pre><code>clip_joined = core.std.Splice([clip1, clip2, clip3])\n</code></pre></p>"},{"location":"basics/howdoi/#how-do-i-stack-two-clips-on-top-of-one-another","title":"How do I stack two clips on top of one another?","text":"<p>This can be done with <code>core.std.StackVertical</code>. But chances are that you are asking this because you want to compare the clips with one another. Unless you want to check if the clips are synced, chances are you want to use multiple output nodes to compare them instead.</p>"},{"location":"basics/howdoi/#how-do-i-interleave-two-clips","title":"How do I interleave two clips?","text":"<p>This can be done with <code>core.std.Interleave</code>. But chances are that you are asking this because you want to compare the clips with one another. Unless you want to check if the clips are synced, chances are you want to use multiple output nodes to compare them instead.</p>"},{"location":"basics/howdoi/#how-do-i-compare-multiple-clips","title":"How do I compare multiple clips?","text":"<p>Set the clips you want to compare as outputs. Then, open the script in vs-preview (see the Setup page) and use the number keys to switch between outputs.</p> vs-toolsVanilla VS <pre><code>from vstools import set_output\n\nset_output(clip1)\nset_output(clip2)\n</code></pre> <pre><code>clip1.set_output(0)\nclip2.set_output(1)\n</code></pre>"},{"location":"basics/howdoi/#how-do-i-name-my-outputs","title":"How do I name my outputs?","text":"<pre><code>from vstools import set_output\n\nset_output(clip1, \"My first clip\")\nset_output(clip2, \"My second clip\")\n</code></pre> <p>Note that the names will only show up in vs-preview and not in other previewers.</p>"},{"location":"basics/howdoi/#how-do-i-preview-a-vfr-clip-with-the-correct-frame-rates","title":"How do I preview a VFR clip with the correct frame rate(s)?","text":"<p>Pass a timecodes file to <code>set_output</code>:</p> <pre><code>from vstools import set_output\n\nset_output(clip, timecodes=\"timecodes.txt\")\n</code></pre> <p>You can also pass a <code>Timecodes</code> object (which you could generate at runtime, or modify): <pre><code>from vstools import set_output, Timecodes\n\ntimecodes = Timecodes.from_file(\"timecodes.txt\")\nset_output(clip, timecodes=timecodes)\n</code></pre></p> <p>You can generate a <code>Timecodes</code> object from a clip's per-frame <code>_DurationNum</code> and <code>_DurationDen</code> properties, but note that this is very slow since it needs to go through the entire clip. One useful method is to generate them once and then save them to a file. <pre><code>import os.path\nfrom vstools import set_output, Timecodes\n\nTIMECODES_NAME = \"timecodes.txt\"\n\nif not os.path.isfile(TIMECODES_NAME):\n    timecodes = Timecodes.from_clip(clip)\n    timecodes.to_file(TIMECODES_NAME)\n\ntimecodes = Timecodes.from_file(TIMECODES_NAME)\nset_output(clip, timecodes=timecodes)\n</code></pre> Remember to regenerate your timecodes file whenever the clip's frames or frame rate change. With the above code, this can be done by just deleting the timecodes file.</p>"},{"location":"basics/howdoi/#how-do-i-get-the-lumachroma-of-a-clip","title":"How do I get the luma/chroma of a clip?","text":"vs-toolsVanilla VS <p><pre><code>from vstools import get_y, get_u, get_v, split, plane\n\ny = get_y(clip)     # Luma\nu = get_u(clip)     # First chroma plane\nv = get_v(clip)     # Second chroma plane\n\n# Alternatively, you can use `split`:\ny, u, v = split(clip)\n\n# Or, you can use the `plane` function:\ny = plane(clip, 0)\nu = plane(clip, 1)\nv = plane(clip, 2)\n</code></pre> If you want to split an RGB clip instead, you'll need to use the equivalent <code>get_r</code>, <code>get_g</code>, <code>get_b</code> functions.</p> <pre><code>y = clip.std.ShufflePlanes(planes=0, colorfamily=vs.GRAY)   # Luma\nu = clip.std.ShufflePlanes(planes=1, colorfamily=vs.GRAY)   # First chroma plane\nv = clip.std.ShufflePlanes(planes=2, colorfamily=vs.GRAY)   # Second chroma plane\n\n# Or, to get all three planes:\ny, u, v = clip.std.SplitPlanes()\n</code></pre> <p>If you only want to see the individual planes of a clip, and not process them, you may want to use vs-preview's \"Split Planes\" plugin instead (see the Setup page).</p>"},{"location":"basics/howdoi/#how-do-i-combine-luma-and-chroma-into-a-clip-or-replace-planes-of-a-clip","title":"How do I combine luma and chroma into a clip, or replace planes of a clip?","text":"vs-toolsVanilla VS <pre><code>from vstools import join\n\n# Join luma and chroma\njoined = join(y, u, v)\n\n# Replace chroma in a YUV clip\nclip_replaced = join(clip, u, v)\n\n# Replace luma in a YUV clip\nclip_replaced = join(y, clip)\n</code></pre> <pre><code># Join luma and chroma\njoined = core.std.ShufflePlanes(clips=[y, u, v], planes=[0, 0, 0], colorfamily=vs.YUV)\n\n# Replace chroma in a YUV clip\nclip_replaced = core.std.ShufflePlanes(clips=[clip, u, v], planes=[0, 0, 0], colorfamily=vs.YUV)\n\n# Replace luma in a YUV clip\nclip_replaced = core.std.ShufflePlanes(clips=[y, clip, clip], planes=[0, 1, 2], colorfamily=vs.YUV)\n</code></pre>"},{"location":"basics/howdoi/#how-do-i-change-a-clips-bit-depth","title":"How do I change a clip's bit depth?","text":"vs-toolsVanilla VS <pre><code>from vstools import depth\n\nclip_i16 = depth(clip, 16)\nclip_float = depth(clip, 32)\n</code></pre> <pre><code>clip_i16 = clip.resize.Point(format=clip.format.replace(bits_per_sample=16, sample_type=vs.INTEGER))\nclip_float = clip.resize.Point(format=clip.format.replace(bits_per_sample=32, sample_type=vs.FLOAT))\n</code></pre> <p>Note that the vanilla VS version does not dither by default, while <code>vstools.depth</code> does dither when necessary. With both versions the dither type can be set in a parameter.</p>"},{"location":"basics/howdoi/#how-do-i-retag-a-clips-color-matrixcolor-rangeetc","title":"How do I retag a clip's color matrix/color range/etc?","text":"<p>Retagging only changes the metadata (here in the form of frame properties) without changing any of the pixel values. (But of course filters called on this clip may behave differently based on the metadata, which is the entire point. In particular your clip will display differently in vs-preview, even though the pixel values are the same.)</p> vs-toolsVanilla VS <pre><code>from vstools import Matrix, ColorRange\n\nclip_retagged = Matrix.BT601.apply(ColorRange.FULL.apply(clip))\n</code></pre> <pre><code>clip_retagged = clip.std.SetFrameProps(_Matrix=vs.MATRIX_BT601, _Range=vs.RANGE_FULL)\n</code></pre>"},{"location":"basics/howdoi/#how-do-i-convert-a-clips-color-matrixcolor-rangeetc","title":"How do I convert a clip's color matrix/color range/etc?","text":"<p>Tag your clip as the source matrix/range/etc and use the <code>core.resize</code> function to convert it to the target matrix/range/etc.</p> <p>Converting color matrix/range/etc will change the pixel values as well as the metadata, so the resulting clip will look the same in a previewer (except for subtle differences due to dithering, etc) even though the pixel values are different.</p> <p><pre><code>from vstools import Matrix, ColorRange  # You can also use Vanilla VS, see above\n\n# Convert a clip from the BT601 matrix to the BT709 matrix\nclip_converted = clip.resize.Point(Matrix.BT601.apply(clip), matrix=Matrix.BT709)\n\n# Convert a clip from limited range to full range\n# Note that you cannot use the ColorRange enum for the `range` argument here!\nclip_converted = clip.resize.Point(ColorRange.LIMITED.apply(clip), range=1)\n</code></pre> Note that the resizing does not dither by default. That does not mean you should dither (usually it's better practice to do most of your filter chain with float clips, and then dither down at the end), but you should be aware of it.</p> <p>Also note that converting color matrix, transfer, or primaries (but not range or chroma location) requires upscaling chroma to the luma's size. The above code assumes a YUV444 clip; it will work with YUV420 clips, but the output will not be good since it uses Point to resize. However, you shouldn't simply replace <code>Point</code> with another scaler like <code>Lanczos</code>, since that scaler would be used for both upscaling and downscaling. It's better to explicitly upscale to YUV444 first (using, say, <code>Lanczos</code>), convert the color space, and then eventually downscale back to YUV420 again (using, say, Hermite, i.e. <code>Bicubic</code> with b=c=0).</p> <p>Warning</p> <p>Do not use the <code>matrix_in</code>/<code>range_in</code>/etc family of arguments to convert color spaces. Frame properties, when present, take precedence over these arguments, which can lead to very unexpected behavior. Hence you should instead be overwriting the frame properties, as done in the snippet above.</p> <p>Warning</p> <p>Color range needs special treatment here, as shown in the above snippet. The meaning of the values <code>0</code> and <code>1</code> is flipped between the <code>_ColorRange</code> frame property and the <code>core.resize</code> function. In the frame property, <code>0</code> means full and <code>1</code> means limited (docs), but in <code>core.resize</code> it's the other way around (docs).</p> <p>Alternatively, you can use fmtconv.</p>"},{"location":"basics/howdoi/#how-do-i-apply-a-filter-to-only-some-frames-in-the-clip","title":"How do I apply a filter to only some frames in the clip?","text":"<p>Unless the filter is a temporal one and you specifically want it to only get your selected frames as an input, the simplest way is to apply the filter to the entire clip and then replace the desired frames afterwards:</p> <pre><code>filtered = clip.std.BoxBlur()\n\n# only blur frames 10 through 19 (excluding frame 20)\npartially_filtered = clip[:10] + filtered[10:20] + clip[20:]\n</code></pre> <p>For more convenience, you can use the <code>replace_ranges</code> function to avoid having to manually slice all the clips. This can also be more performant when you have many ranges to replace.</p> <pre><code>from vstools import replace_ranges\n\n# Replace frames 10 through 19, including 19. `replace_ranges` ranges are inclusive, unless you set `exclusive=True`.\nMY_FRAME_RANGES = [(10, 19)]\n\n# You could also use multiple ranges, and you can use None to denote the left/right end of the clip.\n# Check the function docstring for more info.\n# This would replace frames 10 through 19 and all frames starting from frame 200.\n# MY_FRAME_RANGES = [(10, 19), (200, None)]\n\npartially_filtered = replace_ranges(clip, filtered, MY_FRAME_RANGES)\n</code></pre> <p>Don't worry, even though you give the entire <code>clip</code> as an input to your filter, the filter will not actually run on the frames not added to <code>partially_filtered</code> when you set <code>partially_filtered</code> (or another clip based on it) as an output. Filters are only run on frames when the frame is requested.</p>"},{"location":"basics/howdoi/#how-do-i-decide-at-runtime-whether-to-apply-a-filter-or-not","title":"How do I decide at runtime whether to apply a filter or not?","text":"<p>Unless you want to write your own plugin, the way to do this is with <code>FrameEval</code>:</p> <p>For example, to blur all frames whose average luma is larger than 0.5 (assume the clip is a float clip): <pre><code>blurred = clip.std.BoxBlur()\nstats = clip.std.PlaneStats()\n\ndef blur_some_frames(n, f):\n    if f[0].props[\"PlaneStatsAverage\"] &gt; 0.5:\n        return blurred[n]\n    return f\n\npartially_blurred = clip.std.FrameEval(blur_some_frames)\n</code></pre></p> <p>Try not to instantiate filters inside of the per-frame function, if possible. Note how the above snippet creates the <code>blurred</code> and <code>stats</code> clips outside of the function, and only references them inside the function. This makes the filter only be applied once, instead of once for every frame. Of course you cannot do this if the filter parameters need to vary per frame. In that case, you need to be very careful when your filter's instantiation is very resource-heavy.</p>"},{"location":"basics/howdoi/#how-do-i-apply-a-filter-to-only-a-certain-section-of-the-picture","title":"How do I apply a filter to only a certain section of the picture?","text":"<p>In general this depends very strongly on what filter you're using and what you want to achieve. One common answer, however, is to apply the filter to the entire frame and do a masked merge with the original clip.</p> <p>For example, to only blur a certain rectangle in the frame: <pre><code>from vsmasktools import squaremask\n\n# a 200x200 rectangle whose top left corner is at (500, 500)\nmask = squaremask(clip, 200, 200, 500, 500)\n\nblurred = clip.std.BoxBlur()\n\npartially_blurred = core.std.MaskedMerge(clip, blurred, mask)\n</code></pre> In fact, for the specific purpose of a square mask there exists an even simpler wrapper function (but I showed you the longer snippet to illustrate the general process): <pre><code>from vsmasktools import replace_squaremask\n\nblurred = clip.std.BoxBlur()\npartially_blurred = replace_squaremask(clip, blurred, (200, 200, 500, 500))\n</code></pre> Other ways to build masks include: - Manually building a square mask using a <code>core.std.BlankClip</code> followed by <code>core.std.AddBorders</code>.   There's no real reason to do this except to understand how <code>squaremask</code> might work internally. - Manually building a mask with <code>core.akarin.Expr</code>, using an expression that computes the mask value based on the position. - Building a mask using certain filters (e.g. edge masks) or manual expressions based on the pixel values - Manually drawing a mask in an image editor and importing it from a file - Drawing a mask using subtitle drawings in Aegisub, and rendering the resulting   subtitle line using the <code>core.sub</code> plugin</p>"},{"location":"basics/howdoi/#how-do-i-access-or-modify-a-frames-content-in-python","title":"How do I access or modify a frame's content in Python?","text":"<p>Unless you know what you're doing, chances are that you shouldn't be doing this. Modifying frame contents in Python is slow and not the way VapourSynth is intended to be used. You should instead see if there is a plugin that applies the filter you want to apply, or write such a plugin if there isn't. If you want to apply some custom formula to a frame's pixels, you can use the <code>core.std.Expr</code> or the more powerful third-party <code>core.akarin.Expr</code> functions.</p> <p>That said, accessing frame data from Python can be useful when you're trying out some new filter idea and want to prototype using tools like numpy.</p> <p>To read a frame's contents into a numpy array: <pre><code>import numpy as np\n\n# Get the first plane of frame 100 as a numpy array of shape (clip.height, clip.width)\ndata = np.asarray(clip.get_frame(100)[0])\n</code></pre></p> <p>To modify a frame's contents: <pre><code>def modify(f, n):\n    # Do the necessary imports inside of the function.\n    # Doing them globally can cause issues in some scenarios,\n    # since they may be dropped before the function is executed\n    import numpy as np\n\n    src = np.asarray(f[1][0])\n\n    # The dimensions and dtype of `res` need to match the dimensions and format of `blank` below\n    res = np.zeros((your_target_height, your_target_width), dtype=np.float32)\n\n    # Write some data into `res` here\n\n    dst = f[0].copy()\n    np.copyto(np.asarray(dst[0]), res)\n    return dst\n\n# Set up a blank clip with your desired output width/height/format\n# (which can differ from your input clip's format)\nblank = clip.std.BlankClip(your_target_height, your_target_width, format=vs.GRAYS)\nresult = core.std.ModifyFrame(blank, [blank, clip], modify)\n</code></pre></p>"},{"location":"basics/howdoi/#how-do-i-remove-artifacts-from-a-video-without-being-too-destructive","title":"How do I remove artifacts from a video without being too destructive?","text":"<p>Very carefully.</p>"},{"location":"basics/python-primer/","title":"A Primer on Python","text":"<p>Attention</p> <p>This is NOT a full guide to using Python, and does NOT go into too much depth. This is meant to be a high-level guide to ease learning enough Python to use Vapoursynth with impunity. For more in-depth guides, consult the additional resources section!</p> <p>Getting started with Vapoursynth can seem daunting at first, as it means learning how to write Python scripts. The good news is that Python is one of the simplest programming languages to learn, and you only need to grasp the basics to start filtering videos with Vapoursynth and JET packages.</p>"},{"location":"basics/python-primer/#basic-syntax","title":"Basic syntax","text":"<p>Python is a high-level, interpreted language that emphasizes readability. Its basic syntax is designed to be intuitive and easy to understand, making it accessible even for beginners.</p>"},{"location":"basics/python-primer/#variables","title":"Variables","text":"<p>Variables are a fundamental concept in programming that allow you to store and reference values in your code. Here\u2019s a simple example:</p> <pre><code>a = 1\nb = \"example text\"\n</code></pre> <p>In this example:</p> <ul> <li><code>a</code> is a variable that holds the number 1.</li> <li><code>b</code> is a variable that holds a piece of text,   also known as a \"string\",   which is \"example text.\"</li> </ul> <p>Now, let\u2019s see how this translates to a VapourSynth script. At the beginning of every script, you need to load a video file and assign it to a variable so you can use it throughout your script.</p> <pre><code>src = core.bs.VideoSource(\"a video file.mp4\")\n</code></pre> <p>In this example:</p> <ul> <li>We use the <code>VideoSource</code> function   from the <code>bs</code> module   to load a video file   named \"a video file.mp4\".</li> <li>We assign the loaded video   to the variable <code>src</code>.</li> </ul> <p>Note</p> <p>There are several common conventions that many VapourSynth scripts follow. One example is using the variable name <code>src</code> to represent the source video. Following these conventions is highly recommended, especially when seeking support, as it makes it easier for experienced VapourSynth users to understand the code and assist you!</p> <p>While writing your script, you should be mindful of the variables you write. A common mistake beginners make is to use an older variable when they've filtered a clip and assigned it to a different variable. Take for example:</p> <pre><code>src = core.bs.VideoSource(\"a video file.mp4\")\n\nivtc = core.vfm.VIVTC(src)\ndecimate = core.vfm.VDecimate(src)\n</code></pre> <p>The user is passing <code>src</code> to <code>vfm.VDecimate</code>, when what they likely intended to do was to pass <code>ivtc</code> to it instead. While it's widely considered good practice to use different variables for different filtering operations (as it makes it easier to compare multiple filtered clips), you have to be careful that you assign the correct clip to the function you're calling.</p> <p>This is how it should be called instead:</p> <pre><code>src = core.bs.VideoSource(\"a video file.mp4\")\n\nivtc = core.vfm.VIVTC(src)\ndecimate = core.vfm.VDecimate(ivtc)\n</code></pre>"},{"location":"basics/python-primer/#values","title":"Values","text":"<p>When using functions, you'll often need to pass specific values or parameters. An example of this is the built-in cropping plugin:</p> <pre><code>src = core.std.Crop(src, ...)\n</code></pre> <p>The <code>std.Crop</code> function is used to crop a video clip. It takes parameters for each direction you want to crop. Looking at the VapourSynth documentation, it says the function can be called like this:</p> <pre><code>std.Crop(vnode clip[, int left=0, int right=0, int top=0, int bottom=0])\n</code></pre> <p>Let's pick this function apart:</p> <ul> <li>The first parameter is called <code>clip</code>,   and needs to be a <code>vnode</code>.   This is also known as a <code>VideoNode</code>,   and just means a video clip.</li> <li>Every parameter after this is optional,   which is denoted   by the square brackets.</li> <li>There are four additional optional arguments,   called <code>left</code>, <code>right</code>, <code>top</code>, and <code>bottom</code>.   Each of them expects an integer value.   By default,   the value \"0\" is passed to them.</li> </ul> <p>For example, if we want to crop 100 pixels from the top and right sides of the <code>src</code> clip we loaded earlier, we would pass the following parameters to <code>std.Crop</code>:</p> <pre><code>src = core.std.Crop(src, top=100, right=100)\n</code></pre> <p>You can also pass parameters by their position, though it's oftentimes clearer to specify them by name:</p> <pre><code>#                   clip L  R    T    B\nsrc = core.std.Crop(src, 0, 100, 100, 0)\n</code></pre> <p>A lot of functions don't require you to pass values for it to return something, but in most cases, you'll want to fine-tune these parameters for your source.</p> <p>Using an IDE like Visual Studio Code can help you understand what values to pass by showing expected parameters for functions. IDEs can display parameter names, their types, and default values if the package is properly type-hinted. This is especially true for JET packages, which include type hints.</p> <p>Below is an example of how Visual Studio Code can display the expected parameters for <code>BM3D.denoise</code>:</p> <p></p> <p>Type-Hinting</p> <p>Not all packages are type-hinted. Without type hints, an IDE may not display the types of parameters expected by a function.</p> <p>Here\u2019s a quick guide to common types of values you might encounter:</p> Type Explanation Notes Integer A round number, like 1, 100, 623, etc. Float A decimal number, like 1.0, 5.2, 236.523, etc. String A piece of text encompassed by (double) quotes. For example, \"this is a string\". List A collection of values. May be annotated with the type of values inside it, e.g., list[int]. Sometimes called an Array Dictionary A collection of key/value pairs, e.g., {\"name\": \"value\"}. <p>JET also uses custom types like <code>Kernel</code>s, which need to be imported and created as per the package guidelines. If you need help, resources are often available in documentation or community servers like the JET Discord Server.</p>"},{"location":"basics/python-primer/#installing-and-importing-packages","title":"Installing and importing packages","text":"<p>To use JET tooling and other Python packages, you'll need to install them using pip, Python\u2019s package installer. For detailed steps, refer to the setup guide.</p> <p>Once installed, you can import packages into your Python script using the <code>import</code> statement. There are a couple of ways to do this, depending on how you want to access the functions from the package.</p> <p>If you want to import specific functions, use the <code>from x import y</code> syntax:</p> <pre><code>from vstools import get_y\n</code></pre> <p>This makes the <code>get_y</code> function available to be used directly:</p> <pre><code>src_y = get_y(src)\n</code></pre> <p>You can import multiples functions at once with this syntax:</p> <pre><code>from vstools import get_y, get_u, get_v\n</code></pre> <p>You can also import the entire package:</p> <pre><code>import vstools\n</code></pre> <p>When using this method, you need to reference the function through the package:</p> <pre><code>src_y = vstools.get_y(src)\n</code></pre> <p>You can also assign a custom namespace to your imports for brevity or clarity:</p> <pre><code>from vstools import get_y as gy\n\nsrc_y = gy(src)\n</code></pre> <pre><code>import vstools as vst\n\nsrc_y = vst.get_y(src)\n</code></pre> <p>Custom namespaces</p> <p>Using custom namespaces may make your code cleaner and easier to read for you, but do keep in mind that it may make it more difficult for experienced VapourSynth users to properly support you if you run into any issues!</p>"},{"location":"basics/python-primer/#miscellaneous-information","title":"Miscellaneous information","text":"<p>Some quirks or common VapourSynth script practices don't fit into any one category, so we'll discuss them here.</p>"},{"location":"basics/python-primer/#different-ways-to-call-a-plugin","title":"Different ways to call a plugin","text":"<p><code>core</code> is used for loading plugins. In previous examples, you may have seen it used like so:</p> <pre><code>core.std.Crop(src, ...)\n</code></pre> <p>It's not uncommon for script authors to call plugin functions directly from the clip. This is faster to write, but may make your script a bit harder to understand sometimes.</p> <pre><code>src.std.Crop(...)\n</code></pre> <p>Note that if you call a plugin via this method, you no longer have to pass a <code>vnode</code> as the first argument. This may become a problem when the plugin expects a different type to be passed as its first parameter, such as <code>std.StackHorizontal</code>:</p> <pre><code>std.StackHorizontal(vnode[] clips)\n</code></pre> <p>The square brackets shown here indicate that it expects a list of <code>VideoNode</code>s. Calling <code>src.std.StackHorizontal</code> will only pass the <code>src</code> clip as an argument, making it meaningless to call the function with this method.</p>"},{"location":"basics/python-primer/#additional-learning-resources","title":"Additional learning resources","text":"<p>If you want a more hands-on or practical approach to learning Python, there are several valuable resources available.</p> <p>Here\u2019s a list to get you started:</p> <ul> <li>The Python Tutorial</li> <li>Automate the Boring Stuff with Python by Al Sweigart</li> <li>Video tutorials by Corey Schafer</li> </ul>"},{"location":"basics/setup/","title":"Setup and First Steps","text":"<p>This page explains how to install VapourSynth and the JET packages, as well as how to properly set up your code editor and previewer. If you know what you're doing, you can skip or modify any of the steps listed here, but if you run into issues this should be a fairly foolproof way to get things working again.</p> <p>If you're using Linux, it's very strongly recommended to use Arch Linux or a derivative, as that is the only distribution that provides sufficiently many of the required programs and plugins as packages or via the AUR. On other distributions you risk a great amount of pain compiling a ton of programs and plugins manually. The rest of this page will assume that Linux users use Arch Linux.</p>"},{"location":"basics/setup/#getting-a-clean-slate","title":"Getting a Clean Slate","text":"<p>If you're on Linux, you can probably skip this step.</p> <p>If you have VapourSynth or any version of Python installed, uninstall them. Also delete any leftover directories like</p> <ul> <li><code>%appdata%/VapourSynth</code></li> <li><code>%appdata%/Python</code></li> <li><code>%localappdata%/Programs/VapourSynth</code></li> <li><code>%localappdata%/Programs/Python</code></li> </ul> <p>on Windows.</p>"},{"location":"basics/setup/#installing-python-and-vapoursynth","title":"Installing Python and VapourSynth","text":"WindowsLinux <p>Check the VapourSynth documentation to find out the latest Python version currently supported by VapourSynth, and install it from the Python website. At the time of writing, that's Python 3.12.</p> <p>Then, download the latest GitHub release of VapourSynth and install it.</p> <p>Unless you want to risk running into issues, install the installer versions (not the portable ones) and select \"Install for this user only\" when asked.</p> <p>Install Python and VapourSynth using your package manager. Also install <code>vsgenstubs</code> (<code>vapoursynth-tools-genstubs-git</code> on the AUR)</p>"},{"location":"basics/setup/#installing-vapoursynth-plugins","title":"Installing VapourSynth Plugins","text":"<p>VapourSynth itself only provides a frame server, i.e. the ability to load plugins and use them to process audio and video. The meat of the VapourSynth ecosystem lies in these plugins, which need to be installed separately.</p> <p>If you ever want to run a VapourSynth script or call some function and get an error message like</p> <p><code>No attribute with the name bs exists. Did you mistype a plugin namespace or forget to install a plugin?</code>,</p> <p>this means that you need to install the mentioned plugin, in this case <code>bs</code> (which is short for bestsource).</p> WindowsLinux <p>On Windows, plugins are installed using VSRepo. Open a terminal and run <code>vsrepo.py available</code> to list all installable plugins. To install a plugin (e.g. <code>bs</code>), run <code>vsrepo.py install bs</code>. You can install multiple plugins at once to save time, e.g. <code>vsrepo.py install bs lsmas</code>.</p> <p>Note</p> <p>VSRepo can be finnicky depending on your configuration and file associations. (TODO figure out some foolproof method to call it or fix it upstream) If you run into issues running vsrepo, try using a different terminal (cmd or PowerShell) or calling it as <code>vsrepo</code> or <code>python -m vsrepo</code> rather than <code>vsrepo.py</code>. If all else fails, you can manually navigate to the folder VSRepo is in (usually <code>%localappdata%/Programs/VapourSynth/vsrepo</code>), open a terminal there, and run <code>python vsrepo.py</code> with your arguments. Alternatively, you can also try VSRepoGUI.</p> <p>Install plugins using your package manager, e.g. <code>vapoursynth-plugin-bestssource</code> on Arch Linux. A few plugins are available in the official repositories, but for most of them you'll need to use the AUR.</p> <p>After you've installed a plugin, run <code>vsgenstubs4</code> to generate function stubs for the installed plugins, which will help your language server.</p> <p>To start out, install the following plugins:</p> <ul> <li><code>bs</code> and <code>lsmas</code>, to load audio and video</li> <li><code>akarin</code> and <code>libp2p</code>, which may be needed by vs-preview.</li> </ul>"},{"location":"basics/setup/#installing-the-jet-packages","title":"Installing the JET Packages","text":"<p>The JET python packages build on top of the existing plugin ecosystem to provide</p> <ul> <li>More convenient and Pythonic wrapper functions around various plugins</li> <li>More complex filtering functions which combine functions of various plugins   to achieve various filtering goals</li> <li>vs-preview, a previewer for VapourSynth with plugin support and many useful features for encoders.</li> </ul> <p>To install it, open a terminal and run:</p> <p><code>pip install vsjet</code></p> <p>followed by</p> <p><code>vsjet</code></p> <p>To update your JET packages, you can run <code>vsjet</code> again.</p> <p>If you run into issues with any JET package, first update them to their latest git version by running <code>vsjet latest</code>. This will update to the absolute latest version, but since these can be very bleeding-edge, they're not normally recommended for inexperienced users.</p> <p>Linux users may need to create a virtualenv to install the packages or try their luck with <code>pipx</code>.</p>"},{"location":"basics/setup/#installing-your-code-editor","title":"Installing your Code Editor","text":"<p>You're now ready to use JET packages with VapourSynth. However, it's strongly recommended to also install a code editor or IDE, in order to benefit from a Python language server to see available plugins, functions, documentation, etc. The simplest choice is VS Code:</p> <ul> <li>Install VS Code from its download page</li> <li>Follow the vs-preview documentation   to configure VS Code to work with VapourSynth and vs-preview.</li> </ul>"},{"location":"basics/setup/#opening-your-first-file","title":"Opening your First File","text":"<p>With everything set up, it's time to open your first video in VapourSynth. Start by finding your favorite video file, and copy its path (e.g. <code>C:/Path/to/my/file.mkv</code>). Then, make a file called <code>myscript.vpy</code>, and open it (for example in VS Code). Write the following into it:</p> <pre><code>import vapoursynth as vs\ncore = vs.core\n\nclip = core.lsmas.LWLibavSource(\"C:/Path/to/my/file.mkv\")\n\nclip.set_output(0)\n</code></pre> <p>Where, obviously, you should replace the path with the path to your own video file.</p> <p>Note</p> <p>This example uses the <code>lsmas.LWLibavSource</code> source filter because of its faster indexing. For any kind of more serious work, <code>bs.VideoSource</code> is recommended, since only that filter can fully guarantee accurate seeking.</p> <p>Then, open this file in vs-preview. If you've correctly set up VS Code, you should be able to just press F5. Otherwise, you can also open a terminal in your script's directory and run <code>vspreview myscript.vpy</code>.</p> <p>You should see vs-preview open and display your video.</p>"},{"location":"basics/setup/#a-few-further-first-steps","title":"A few further First Steps","text":"<p>With this, you've learned how to install everything you need. The following will explain a few basic first steps if this is your first time using VapourSynth or vs-preview.</p>"},{"location":"basics/setup/#getting-comfortable-with-vs-preview","title":"Getting Comfortable with vs-preview","text":"<p>If you've followed the instructions above, you should now have vs-preview opened and should be able to preview your video file. Here are a few things you can try out:</p> <ul> <li>Press Space to play or pause</li> <li>Click around in the timeline bar below the video display to step around the video</li> <li>Ctrl+Scroll to zoom and Click+Drag to pan around the displayed image</li> <li>Click the \"Pipette\" button at the bottom and look at the values at the bottom changing while you move your mouse around the image.</li> <li>Click the \"Benchmark\" button at the bottom and click \"Run\" to find out how fast your VapourSynth script runs.     At the moment, your script just loads a video, so it should be fairly fast,     but in the future your scripts might contain more complex filtering, and knowing how fast or slow your filtering is will be more important.</li> <li>Click the \"Comp\" button at the bottom and click \"Start Upload\".     Once the upload is done, find the box containing a <code>slow.pics</code> link and press the button next to it to copy that link to your clipboard.     Open that link in your browser: You'll get a <code>slow.pics</code> comparison of random frames in your video.     If your script has multiple outputs, the comparison will show all output nodes.</li> <li> <p>Move your mouse to the very right of vs-preview's window and drag the bar there to the left.     This opens the plugins panel, which contains one tab for each vs-preview plugin.     First, open the \"Frame Props\" tab and have a look at the values there.</p> <p>Then, open the \"Split Planes\" tab. This shows the individual planes of your video. For your average video clip, this will consist of one luma plane and two chroma planes with half the width and height. You can press Ctrl+A to unlock the split planes view, which will allow you to freely zoom and pan around in the view like you would on the normal video.</p> </li> </ul>"},{"location":"basics/setup/#a-second-output-node","title":"A Second Output Node","text":"<p>Now, let's do some actual filtering. Add two lines to the bottom of your VapourSynth script, so that it looks as follows:</p> <pre><code>import vapoursynth as vs\ncore = vs.core\n\nclip = core.lsmas.LWLibavSource(\"C:/Path/to/my/file.mkv\")\n\nclip.set_output(0)\n\nblurred = core.std.BoxBlur(clip)\n\nblurred.set_output(1)\n</code></pre> <p>Then press Ctrl+R to reload vs-preview.</p> <p>Note</p> <p>Reloading vs-preview in-place with Ctrl+R uses dark magic and can occasionally break with certain complex scripts. If you run into issues with reloading, the foolproof way is always to close and reopen vs-preview.</p> <p>If you open the drop-down at the bottom left of vs-preview, you should now see two output nodes you can switch between. The first is your video, the second is a slightly blurred version of your video. You can also press the 1 and 2 keys to switch between them (which is the recommended method since it's much faster).</p>"},{"location":"basics/setup/#going-jet","title":"Going JET","text":"<p>The above example code only used standard VapourSynth functions; except for vs-preview we didn't use any JET packages. But even in such a simple script, JET packages can save you a bit of work. Let's see how this script could be modified:</p> <pre><code>from vstools import core, set_output\n\nclip = core.lsmas.LWLibavSource(\"C:/Path/to/my/file.mkv\")\nblurred = core.std.BoxBlur(clip)\n\nset_output(clip)\nset_output(blurred)\n</code></pre> <p>Let's go over the differences:</p> <ol> <li>The top lines are different.    While standard VapourSynth scripts start with the incantations <code>import vapoursynth as vs</code> and <code>core = vs.core</code>,    the JET way is <code>from vstools import core, set_output</code> (possibly followed by further imports).    This is not a big difference, but it does cut down the boilerplate to one line.</li> <li>We no longer need to give numbers to <code>set_output</code>.    When using <code>clip.set_output</code>, you need to give <code>set_output</code> a number to specify what output node the given clip should be.    When you want to add another node at the start, you'd need to update the numbers of all following output nodes, which can be annoying.    The <code>set_output</code> function in vs-tools automatically numbers nodes based on the order they're output in.</li> <li>Nodes are automatically named.    If you open the above script in vs-preview, you'll see that the dropdown in the bottom left now contains the names <code>clip</code> and <code>blurred</code>    instead of <code>Video Node 1</code> and <code>Video Node 2</code>, matching how the clips were called in our script.    If you were to upload a comparison to slow.pics, the images in the comparison would also be labeled like this.</li> </ol> <p>We can also name nodes manually by passing another argument to <code>set_output</code>:</p> <pre><code>set_output(clip, \"Source\")\nset_output(blurred, \"Blurred\")\n</code></pre> <p>If you wanted to, you could also replace the <code>BoxBlur</code> call with a JET wrapper:</p> <pre><code>from vstools import core, set_output\nfrom vsrgtools import box_blur\n\nclip = core.lsmas.LWLibavSource(\"C:/Path/to/my/file.mkv\")\nblurred = box_blur(clip)\n\nset_output(clip, \"Source\")\nset_output(blurred, \"Blurred\")\n</code></pre> <p>Whether you want to do this or not is a matter of taste. The advantage of writing <code>core.std.BoxBlur</code> explicitly is that you see exactly what plugin is called and don't need to worry about understanding the Python wrapper. On the other hand, a wrapper like <code>box_blur</code> may have more features (e.g. here <code>box_blur</code> allows specifying a different radius for every plane, which <code>core.std.BoxBlur</code> doesn't allow out-of-the-box`) and be easier to use.</p>"},{"location":"basics/setup/#using-a-better-source-filter","title":"Using a Better Source Filter","text":"<p>Finally, I need to talk about the line</p> <pre><code>clip = core.lsmas.LWLibavSource(\"C:/Path/to/my/file.mkv\")\n</code></pre> <p>This is the line that loads your video using the <code>lsmas</code> plugin. The <code>LWLibavSource</code> video source is very reliable for most videos you'll encounter, but it's not infallible. When jumping around the video, it's possible for such source filters to sometimes return the wrong frame, which can render your encodes unusable. To avoid this, the source filter BestSource is recommended. In exchange for needing a long time to process when opening a video for the first time, this filter ensures perfect seeking accuracy in audio and video. You can call it using</p> <pre><code>clip = core.bs.VideoSource(\"C:/Path/to/my/file.mkv\", showprogress=True)\n</code></pre> <p>Don't be surprised if vs-preview takes a long time to open now. This is necessary in order for BestSource to be accurate. The second time you open the same video, it will be much faster.</p> <p>LWLibavSource is still fine when you want to quickly look through some video, but as soon as you want to do any kind of actual encoding, it's recommended to use BestSource.</p>"},{"location":"encoding/x264params/","title":"x264 Parameters","text":""},{"location":"encoding/x264params/#general-settings","title":"General settings","text":"<p>These are the settings that you shouldn't touch between encodes.</p>"},{"location":"encoding/x264params/#preset","title":"Preset","text":"<p>Presets apply a number of parameters, which can be referenced here Just use the placebo preset, we'll change the really slow stuff, anyway:</p> <pre><code>--preset placebo\n</code></pre>"},{"location":"encoding/x264params/#level","title":"Level","text":"<p>Where --preset applies a defined set of parameters, --level provides a set of limitations to ensure decoder compatibility. For further reading, see this Wikipedia article</p> <p>For general hardware support level 4.1 is recommended, otherwise you may omit this.</p> <pre><code>--level 41\n</code></pre>"},{"location":"encoding/x264params/#motion-estimation","title":"Motion estimation","text":"<p>For further reading see this excellent thread on Doom9</p> <p>x264 has two motion estimation algorithms worth using, umh and tesa. The latter is a placebo option that's really, really slow, and seldom yields better results, so only use it if you don't care about encode time. Otherwise, just default to umh:</p> <pre><code>--me umh\n</code></pre>"},{"location":"encoding/x264params/#ratecontrol-lookahead","title":"Ratecontrol lookahead","text":"<p>The ratecontrol lookahead (rc-lookahead) setting determines how far ahead the video buffer verifier (VBV) and macroblock tree (mbtree) can look. Raising this can slightly increase memory use, but it's generally best to leave this as high as possible:</p> <pre><code>--rc-lookahead 250\n</code></pre> <p>If you're low on memory, you can lower it to e.g. 60.</p>"},{"location":"encoding/x264params/#source-specific-settings","title":"Source-specific settings","text":"<p>These settings should be tested and/or adjusted for every encode.</p>"},{"location":"encoding/x264params/#ratecontrol","title":"Ratecontrol","text":"<p>Beyond all else, this is the single most important factor for determining the quality from any given input. Due to how poorly x264 handles lower bitrates (comparatively, particularly when encoding 8-bit) starving your encode will result in immediate artifacts observable even under the lightest scrutiny.</p> <p>While manipulating other settings may make small, but usually noticeable differences; an encode can't look great unless given enough bits.</p> <p>For some further insight, reference this article</p>"},{"location":"encoding/x264params/#constant-ratefactor","title":"Constant ratefactor","text":"<p>For more information please see this post by an x264 developer.</p> <p>The constant ratefactor (CRF) is the suggested mode for encoding. Rather than specifying a target bitrate, CRF attempts to ensure a consistent quality across a number of frames; as such, the resulting bitrate will only be consistent if passed identical values. In short, CRF is recommended for use with your finalized encode, not testing, where two pass is recommended.</p> <p>Lower CRF values are higher quality, higher values lower. Some settings will have a very large effect on the bitrate at a constant CRF, so it's hard to recommend a CRF range, but most encodes will use a value between 15.0 and 20.0. It's best to test your CRF first to find an ideal bitrate, then test it again after all other settings have been tested with 2pass.</p> <p>To specify CRF:</p> <pre><code>--crf 16.9\n</code></pre>"},{"location":"encoding/x264params/#two-pass","title":"Two pass","text":"<p>An alternative to CRF which leverages an initial pass to collect information about the input before encoding. This comes with two distinct advantages:</p> <ul> <li>The ability to target a specific bitrate</li> <li>Effectively infinite lookahead</li> </ul> <p>This is very suitable for testing settings, as you will always end up at almost the same bitrate no matter what.</p> <p>As mentioned previously, to encode this two runs are necessary. The first pass can be sent to /dev/null, since all we care about is the stats file. To specify which of the two passes you're encoding all we need to change is <code>--pass</code></p> <pre><code>vspipe -c y4m script.vpy - | x264 --demuxer y4m --preset placebo --pass 1 --bitrate 8500 -o /dev/null -\n</code></pre> <pre><code>vspipe -c y4m script.vpy - | x264 --demuxer y4m --preset placebo --pass 2 --bitrate 8500 -o out.h264 -\n</code></pre>"},{"location":"encoding/x264params/#chroma-quantizer-offset","title":"Chroma quantizer offset","text":"<p>If you're struggling with chroma getting overly distorted, it can be worth tinkering with this option. You can find examples of what bitstarved chroma can look like HERE and HERE. Lower values give more bits to chroma, higher values will take away. By default, this will be -2 for 4:2:0 and -6 for 4:4:4. Setting this will add your offset onto -2.</p> <p>To lower the chroma QP offset from -2 to -3, granting chroma more bits:</p> <pre><code>--chroma-qp-offset -1\n</code></pre>"},{"location":"encoding/x264params/#deblock","title":"Deblock","text":"<p>For an explanation of what deblock does, read this Doom9 post and this blog post</p> <p>Set this too high and you'll have a blurry encode, set it too low and you'll have an overly blocky encode. We recommend testing deblock in a range of <code>-2:-2</code> to <code>0:0</code> (animated content should use stronger deblock settings). Usually, you can test this with the same alpha and beta parameters at first, then test offsets of \u00b11.</p> <p>Many people will mindlessly use -3:-3, but this tends to lead to unnecessary blocking that could've been avoided had this setting been tested.</p> <p>To specify e.g. an alpha of -2 and a beta of -1:</p> <pre><code>--deblock -2:-1\n</code></pre>"},{"location":"encoding/x264params/#quantizer-curve-compression","title":"Quantizer curve compression","text":"<p>The quantizer curve compression (qcomp) is effectively the setting that determines how bits are distributed among the whole encode. It has a range of 0 to 1, where 0 is a constant bitrate and 1 is a constant quantizer, the opposite of a constant bitrate. This means qcomp affects how much bitrate you're giving to \"important\" scenes as opposed to \"unimportant\" scenes. In other words, it can be seen as the trade-off between bitrate allocation to simple or static scenes and complex or high motion scenes. Higher qcomp will allocate more to the latter, lower to the former.</p> <p>It's usually recommended to set this between <code>0.60</code> and <code>0.70</code> without mbtree and <code>0.70</code> and <code>0.85</code> with mbtree. You want to find that sweet spot where complex scenes will look good enough without ruining simple scenes.</p> <pre><code>--qcomp 0.60\n</code></pre>"},{"location":"encoding/x264params/#macroblock-tree","title":"Macroblock tree","text":"<p>From this thread by an x264 developer: \"It tracks the propagation of information from future blocks to past blocks across motion vectors. It could be described as localizing qcomp to act on individual blocks instead of whole scenes. Thus instead of lowering quality in high-complexity scenes (like x264 currently does), it'll only lower quality on the complex part of the scene, while for example a static background will remain high-quality. It also has many other more subtle effects, some potentially negative, most probably not.\"</p> <p>Curious readers can reference the paper directly</p> <p>Macroblock tree ratecontrol (mbtree) can lead to large savings for very flat content, but tends to be destructive on anything with a decent amount of grain. If you're encoding something with very little movement and variation, especially cartoons and less grainy digital anime, it's recommended to test this setting to see if it's worth it.</p> <p>When using mbtree, you should max out your lookahead (<code>--rc-lookahead 250</code>) and use a high qcomp \u22650.70.</p>"},{"location":"encoding/x264params/#adaptive-quantization","title":"Adaptive quantization","text":"<p>While qcomp determines bit allocation for frames across the video, adaptive quantization (AQ) can be seen as in charge of doing this on a block-basis, i.e. throughout a frame. It does so by distributing bits from higher contrast areas to lower contrast regions. This is done because, while lower contrast areas are mathematically less significant, the human eye considers them of equal importance.</p> <p>For more on this, the explanation in this document is quite helpful. This post grants a bit of insight on the implementation in x264.</p> <p>There are three modes available in vanilla x264:</p> <ol> <li>Allow AQ to redistribute bits across the whole video and within frames.</li> <li>Auto-variance; this attempts to adapt strength per-frame.</li> <li>Auto-variance with a bias to dark scenes.</li> </ol> <p>Generally speaking, you'll likely get the best results with AQ mode 3. With the other two modes, you have to carefully make sure that darks aren't being damaged too much. If you e.g. have a source without any dark scenes (or only very few), it can be worth manually allocating more bits to darks via zoning and using AQ modes 1 or 2.</p> <p>This comes along with a strength parameter. For modes 1 and 2, you usually want a strength between 0.80 and 1.30. Mode 3 is a bit more aggressive and usually looks best with a strength between 0.60 and 0.85.</p> <p>Raising the AQ strength will help flatter areas, e.g. by maintaining smaller grain and dither to alleviate banding. However, higher AQ strengths will tend to distort edges more.</p> <p>Older, grainier live action content will usually benefit more from lower AQ strengths and may benefit less from the dark scene bias present in AQ mode 3, while newer live action tends to benefit more from higher values. For animation, this setting can be very tricky; as both banding and distorted edges are more noticeable. It's usually recommended to run a slightly lower AQ strength, e.g. around 0.60 to 0.70 with mode 3.</p> <p>To use e.g. AQ mode 3 with strength 0.80:</p> <pre><code>--aq-mode 3 --aq-strength 0.80\n</code></pre>"},{"location":"encoding/x264params/#aq-dark-bias-strength","title":"AQ dark bias strength","text":"<p>Some mods now include a parameter to control AQ mode 3's bias to dark scenes strength. With this, the dark bias strength is a multiple of the AQ strength, meaning higher bias strength raises the dark bias and lower bias strength trends towards AQ mode 2.</p> <p>This can be very useful in cases where you want to give more or fewer bits to darks without affecting the AQ strength. To apply an AQ dark bias strength of 1.00:</p> <pre><code>--aq-mode 3 --aq-bias-strength 1.00\n</code></pre>"},{"location":"encoding/x264params/#motion-estimation-range","title":"Motion estimation range","text":"<p>The motion estimation range (merange) determines how many pixels are used for motion estimation. Larger numbers will be slower, but can be more accurate for higher resolutions. However, go too high, and the encoder will start picking up unwanted info, which in turn will harm coding efficiency. You can usually get by with testing <code>32</code>, <code>48</code>, and <code>64</code>, then using the best looking one, preferring lower numbers if equivalent:</p> <pre><code>--merange 32\n</code></pre>"},{"location":"encoding/x264params/#frame-type-quantizer-ratio","title":"Frame type quantizer ratio","text":"<p>To understand this parameter, one needs to know what the different frame types are. The Wikipedia article on this subject is very useful for this.</p> <p>These settings determine how bits are distributed among the different frame types. Generally speaking, you want to have an I-frame to P-frame ratio (ipratio) around 0.10 higher than your P-frame to B-frame ratio (pbratio). Usually, you'll want to lower these from the defaults of <code>1.40</code> for ipratio and <code>1.30</code> for pbratio, although not by more than 20.</p> <p>Lower ratios will tend to help with grainier content, where less information from previous frames can be used, while higher ratios will usually lead to better results with flatter content.</p> <p>You can use the stats created by the x264 log at the end of the encoding process to check whether the encoder is over-allocating bits to a certain frame type and investigate whether this is a problem. A good guideline is for P-frames to be double the size of B-frames and I-frames in turn be double the size of P-frames. However, don't just blindly set your ratios so that this is the case. Always use your eyes.</p> <p>To set an ipratio of <code>1.30</code> and a pbratio of <code>1.20</code>:</p> <pre><code>--ipratio 1.30 --pbratio 1.20\n</code></pre> <p>If using mbtree, pbratio doesn't do anything, so only test and set ipratio.</p>"},{"location":"encoding/x264params/#psychovisually-optimized-rate-distortion-optimization","title":"Psychovisually optimized rate-distortion optimization","text":"<p>One big issue with immature encoders is that they don't offer psychovisual optimizations like psy-rdo. What it does is distort the frame slightly, sharpening it in the process. This will make it statistically less similar to the original frame, but will look better and more similar to the input. What this means is this is a weak sharpener of sorts, but a very much necessary sharpener!</p> <p>The setting in x264 comes with two options, psy-rdo and psy-trellis, which are both set via the same option:</p> <pre><code>--psy-rd rdo:trellis\n</code></pre> <p>Unfortunately, the latter will usually do more harm than good, so it's best left off. The psy-rdo strength should be higher for sharper content and lower for blurrier content. For animated content, psy-rdo can introduce ringing even with default values. We suggest using lower values, between  <code>0.60</code> and <code>0.90</code>. For live action content where this is of much lesser concern you should find success with values around <code>0.95</code> to <code>1.10</code>. </p> <p>When testing this, pay attention to whether content looks sharp enough or too sharp, as well as whether anything gets distorted during the sharpening process.</p> <p>For example, to set a psy-rd of 1.00 and psy-trellis of 0:</p> <pre><code>--psy-rd 1.00:0\n</code></pre>"},{"location":"encoding/x264params/#dct-block-decimation","title":"DCT block decimation","text":"<p>Disabling DCT block decimation (no-dct-decimate) is very common practice, as it drops blocks deemed unimportant. This importance decision is made by checking if there are enough nonzero large coefficients, i.e. whether higher frequencies are present. For high quality encoding, this is often unwanted and disabling this is wise. However, for flatter content, leaving this on can aid with compression. Just quickly test on and off if you're encoding something flat.</p> <p>To disable DCT block decimation:</p> <pre><code>--no-dct-decimate\n</code></pre>"},{"location":"encoding/x264params/#video-buffer-verifier","title":"Video buffer verifier","text":"<p>To understand what this is, there's actually a Wikipedia article you can read. Alternatively, you may find this video presentation from demuxed informative.</p> <p>For us, the main relevance is that we want to disable this when testing settings, as video encoded with VBV enabled will be non-deterministic. Otherwise, just leave it at your level's defaults.</p> <p>To disable VBV:</p> <pre><code>--vbv-bufsize 0 --vbv-maxrate 0\n</code></pre> <p>VBV settings for general hardware compliance (High@L4.1) <pre><code>--vbv-bufsize 78125 --vbv-maxrate 62500\n</code></pre></p>"},{"location":"encoding/x264params/#reference-frames","title":"Reference frames","text":"<p>The reference frames (ref) setting determines how many frames P frames can use as reference. Many existing guides may provide an incorrect formula to find the 'correct' value. Do not use this. Rather, allow x264 to calculate this for automatically (as dictated by <code>--level</code>). </p> <p>Otherwise, if you don't care about compatibility with 15 year old TVs and 30 year old receivers, set this however high you can bare, with a maximum value of 16. Higher refs will improve encoder efficiency at the cost of increased compute time.</p> <p>To set the maximum value of 16:</p> <pre><code>--ref 16\n</code></pre>"},{"location":"encoding/x264params/#zones","title":"Zones","text":"<p>Sometimes, the encoder might have trouble distributing enough bits to certain frames, e.g. ones with wildly different visuals or sensitive to banding. To help with this, one can zone out these scenes and change the settings used to encode them.</p> <p>When using this to adjust bitrate, one can specify a CRF for the zone or a bitrate multiplier. It's very important to not bloat these zones, e.g. by trying to maintain all the grain added while debanding. Sane values tend to be \u00b12 from base CRF or bitrate multipliers between <code>0.75</code> and <code>1.5</code>.</p> <p>To specify a CRF of 15 for frames 100 through 200 and 16 for frames 300 through 400, as well as a bitrate multiplier of 1.5 for frames 500 through 600:</p> <pre><code>--zones 100,200,crf=15/300,400,crf=16/500,600,b=1.5\n</code></pre> <p>For a more complete picture of what --zones can and can not manipulate, see this section.</p>"},{"location":"encoding/x264params/#output-depth-and-color-space","title":"Output depth and color space","text":"<p>To encode 10-bit and/or 4:4:4 video, one must specify this via the following parameters:</p> <pre><code>--output-depth 10 --output-csp i444\n</code></pre>"},{"location":"filtering/dehardsubbing/","title":"Dehardsubbing","text":"<p>Hardsubs are subtitles that are a part of the video, i.e. they are \"burned-in.\" This is in contrast to softsubs which are kept separate from the video and thus can be toggled or edited without requiring a new encode.</p> <p>This page will go over how to dehardsub (remove hardsubs) from a video.</p>"},{"location":"filtering/dehardsubbing/#prerequisite","title":"Prerequisite","text":"<p>Dehardsubbing requires a secondary video source that is not hardsubbed in order to fill in the details that the hardsubs of the primary video source replaced. The secondary source is called the \"reference\" or \"ref\" clip in the vs-masktools APIs that will be shown here.</p> <p>DEAD DEAD DEMONS DEDEDEDE DESTRUCTION (2024) will be used as a contemporary example of this. The WEB release of this movie/ONA has two video sources: the CR Japanese stream with no (English) hardsubs, and the CR English stream with hardsubs.</p>"},{"location":"filtering/dehardsubbing/#getting-started","title":"Getting Started","text":"<p>Here is an example of a hardsub that one might want to dehardsub, because it doesn't look particularly good and a typesetter could redo it via softsubs:</p> HardsubbedReference <p></p> <p></p> <p>Since a video without hardsubs (the reference clip here) is available, one solution is to simply replace this hardsubbed scene with its counterpart:</p> <pre><code>from vstools import replace_ranges\n\ndehardsub = replace_ranges(hardsub, ref, (1138, 1221))\n</code></pre> <p>This will replace frames 1138 through 1221 (inclusive) of the hardsubbed clip <code>hardsub</code> with the reference clip <code>ref</code>. This is a fine solution if both sources are of equal quality and nothing of value is being lost by throwing away this whole range of frames from <code>hardsub</code>.</p> <p>But let's say that <code>hardsub</code> is a better video source than <code>ref</code>, or that these sources are being Lehmer-merged, or that for any other reason it is undesirable to replace whole frames like this if it can be avoided. A better solution then would be to replace only the hardsubbed region of the scene. This can be accomplished with classes like <code>HardsubSign</code>:</p> <pre><code>from vsmasktools import HardsubSign\n\ndehardsub = HardsubSign(ranges=[(1138, 1221)]).apply_dehardsub(hardsub, ref)\n\n# Can optionally get the mask to preview or modify it.\ndehardsub_mask = HardsubSign(ranges=[(1138, 1221)]).get_mask(hardsub, ref)\n</code></pre> <p>This will find the differences between the <code>hardsub</code> and <code>ref</code> clips, for the same frame range as before, and apply a mask that replaces just the hardsubbed region of <code>hardsub</code> with the respective region from <code>ref</code>.</p> HardsubbedHardsub mask <p></p> <p></p>"},{"location":"filtering/dehardsubbing/#bounding","title":"Bounding","text":"<p>This next example demonstrates two kinds of hardsubs: the good and the bad. Here, the news broadcast's overlay is actually good (for an English release, at least) and leaving it hardsubbed would save a typesetter some time. However, this scene also has the bad Arial signs that could be redone better in softsubs if they're important. If one tries to use <code>HardsubSign</code> like before on this, it will mask everything.</p> HardsubbedReferenceHardsub mask <p></p> <p></p> <p></p> <p>Since the bad hardsubs are all contained to a single region for the whole scene, the mask can be bounded by a <code>BoundingBox</code>. Think of this as masking the mask.</p> <pre><code>from vsmasktools import BoundingBox, HardsubSign\n\ndehardsub = HardsubSign(\n    ranges=[(13784, 13843)],\n    bound=BoundingBox(pos=(0, 0), size=(1230, 880))\n).apply_dehardsub(hardsub, ref)\n</code></pre> <p>The bound is drawn from coordinate (0, 0) (the top-left corner) to a size of 1230x880, which is enough to cover only the Arial signs.</p> HardsubbedHardsub maskBounded hardsub maskDehardsubbed <p></p> <p></p> <p></p> <p></p>"},{"location":"filtering/dehardsubbing/#fades","title":"Fades","text":"<p>Hardsubs that fade in or out may require special handling to account for when the hardsub is at its peak transparency. Lowering the threshold parameter <code>thr</code> of <code>HardsubSign</code> may help but this can also begin to catch differences in noise between the clips. This example will use <code>HardsubSignFades</code>, a specialized version of <code>HardsubSign</code>, on the first frame of a fade-in since that is the most difficult part of the sequence to mask.</p> <pre><code>from vsmasktools import HardsubSignFades\nfrom vstools import get_y, replace_ranges\n\ndehardsub_mask = HardsubSignFades().get_mask(hardsub, ref)\ndehardsub = hardsub.std.MaskedMerge(ref, get_y(dehardsub_mask))\ndehardsub = replace_ranges(hardsub, dehardsub, (1462, 1524))\n</code></pre> HardsubbedReferenceHardsub maskHardsub fade mask <p></p> <p></p> <p></p> <p></p> <p>Notice how just with default parameters, <code>HardsubSignFades</code> is able to catch more of the hardsubs than plain <code>HardsubSign</code> can. This snippet also demonstrates how it is not necessary to use the <code>apply_dehardsub()</code> function; instead one can call <code>get_mask()</code> to get the dehardsub mask for any further processing before applying it themselves. Don't forget to <code>replace_ranges()</code> just the hardsubbed range(s) afterwards, since that was previously being handled by <code>apply_dehardsub()</code>.</p>"},{"location":"filtering/dehardsubbing/#combining","title":"Combining","text":"<p>When there are multiple hardsubs across the whole clip, there are a few convenient ways of applying all the masks without needing to duplicate the above snippets for each hardsub.</p> <p>The first method is to take advantage of the <code>ranges</code> parameter, so long as all of the respective hardsubs can be handled in the same way.</p> <pre><code>from vsmasktools import HardsubSign\n\n# Frame ranges (inclusive) that contain hardsubs.\nHARDSUBS = [\n    (1138, 1221),\n    (11045, 11143),\n    (14079, 14173),\n    # etc.\n]\n\ndehardsub = HardsubSign(ranges=HARDSUBS).apply_dehardsub(hardsub, ref)\n</code></pre> <p>The remaining two methods are more flexible in that they allow for combining different types of <code>HardsubSign</code>. In this example there are regular hardsubs and fading hardsubs:</p> <pre><code>from vsmasktools import HardsubSign, HardsubSignFades\n\n# Frame ranges (inclusive) that contain regular hardsubs.\nHARDSUBS = [\n    (1138, 1221),\n    (11045, 11143),\n    (14079, 14173),\n    # etc.\n]\n\n# Frame ranges (inclusive) that contain fading hardsubs.\nHARDSUB_FADES = [\n    (1462, 1524),\n    (9459, 9511),\n    # etc.\n]\n\nregular_dehardsub = HardsubSign(ranges=HARDSUBS)\nfading_dehardsub = HardsubSignFades(ranges=HARDSUB_FADES)\n</code></pre> <p>From here, one can either use <code>get_all_sign_masks()</code> to get a composite mask containing everything:</p> <pre><code>from vsmasktools import get_all_sign_masks\n\ndehardsub_mask = get_all_sign_masks(hardsub, ref, [regular_dehardsub, fading_dehardsub])\ndehardsub = hardsub.std.MaskedMerge(ref, dehardsub_mask)\n</code></pre> <p>Or sequentially apply each mask with <code>bounded_dehardsub()</code>:</p> <pre><code>from vsmasktools import bounded_dehardsub\n\ndehardsub = bounded_dehardsub(hardsub, ref, [regular_dehardsub, fading_dehardsub])\n</code></pre>"},{"location":"filtering/order/","title":"Order of filtering operations","text":"<p>Deciding which filters to use is important, but what's arguably even more important is understanding which order to run filters in.</p>"},{"location":"filtering/order/#inverse-filters","title":"Inverse filters","text":"<p>If you're running any filter that does the inverse of a process applied by the studio or authoring company, such as a descale or lowpass filter, you should almost always perform those before any other filtering. These types of filters rely on the \"state\" of the video being the same or as close as possible to the state when the filter was applied.</p> <p>For example, applying anti-aliasing before descaling will modify the line art too much, making it difficult to reverse accurately. This may seem obvious, but issues arise when you intend to run multiple filters that do the inverse of different operations.</p> <p>A common example would be an HD source that has been both upscaled and telecined. The trick to dealing with these is to imagine the original process. In this example, it's easy to imagine that telecining depends on the interlacing being 1px tall. Otherwise, it wouldn't be reversible by a player. This implies that, from a production standpoint, telecining must be performed last. Therefore, if you want to undo an upscale on a telecined source, you must IVTC (inverse telecine) first, unless you have reason to believe this was not the original order of operations.</p> <p>Another example would be a source that is lowpassed and telecined. In this case, you must do a bit more detective work, as it could reasonably be done in either order. If there's significant vertical ringing that causes issues during inverse telecining, you may have to address the frequency merging first. However, if your source only has horizontal ringing, as is the case with many R2J DVDs, the order of operations matters less. This is because neither filter will interfere with how you would fix the artifact the other is trying to resolve.</p>"},{"location":"filtering/order/#generic-filtering","title":"Generic filtering","text":"<p>For most modern titles, there's a generally accepted order for applying filters in a filterchain:</p> <p></p>"},{"location":"filtering/order/#descaling","title":"Descaling","text":"<p>Attention</p> <p>NEVER descale a video unless you're absolutely certain your descale is correct.</p> <p>Descaling and similar inverse operations should be performed first if the source can be safely descaled, for reasons outlined previously.</p>"},{"location":"filtering/order/#denoising","title":"Denoising","text":"<p>Since every consumer source contains some degree of compression noise, denoising is recommended. You may perform this step prior to descaling if the studio or authoring company added strong dithering in post that interferes with descaling. Early denoising can also speed up certain filterchains.</p>"},{"location":"filtering/order/#anti-aliasing","title":"Anti-aliasing","text":"<p>Attention</p> <p>A proper rescale will often deal with aliasing, ringing, and haloing artifacting. NEVER perform any of those three steps unless you're absolutely certain you must.</p> <p>Noise around edges may affect anti-aliasing, so perform this step after denoising. Anti-aliasing should ideally be scene-filtered because it's a highly destructive filter and can significantly slow down the filtering process. Only perform this step if your source actually has aliasing.</p>"},{"location":"filtering/order/#deringingdehaloing","title":"Deringing/Dehaloing","text":"<p>Many top anti-aliasing filters might introduce mild ringing or haloing under certain conditions. It's safer to dering or dehalo after anti-aliasing. These are destructive processes, so only use them when absolutely necessary.</p>"},{"location":"filtering/order/#debanding","title":"Debanding","text":"<p>Similar to denoising, nearly every source exhibits some degree of banding due to compression. Avoid excessive debanding to prevent unnecessary detail loss. A mild protective deband can be beneficial if minimal banding is present.</p>"},{"location":"filtering/order/#redithering","title":"Redithering","text":"<p>Redithering aids in preserving gradients during encoding, especially since even 10-bit video doesn't have enough color depth to maintain all gradients flawlessly. Debanding and denoising may disrupt existing dither patterns, causing them to appear \"broken\" or \"splotchy\". Reapplying dither using a grain function that mimics the source's pattern can help smoothen out these issues. It's always recommended to apply dynamic dithering, even if the original source used static dithering.</p>"},{"location":"filtering/descaling/descaling/","title":"Descaling","text":"<p>Descaling can be one of the most powerful filtering techniques when used correctly, but it can also be extremely destructive when used incorrectly. This page introduces descaling and outlines the current state of knowledge surrounding methods to find and judge descale parameters.</p>"},{"location":"filtering/descaling/descaling/#what-is-descaling","title":"What is Descaling?","text":"<p>Descaling is the process of mathematically inverting an upscaling operation. Since conventional upscaling algorithms are linear (when ignoring rounding and clipping) and separable, this can be done fairly efficiently by solving a system of linear equations, provided that the parameters that were used for the upscale are known.</p> <p>Descaling itself is easy; it's fully handled by the <code>descale</code> plugin. The much, much harder part is finding the parameters to descale with, and ensuring that they are correct. More often than not, the best decision is not to descale at all.</p>"},{"location":"filtering/descaling/descaling/#why-is-descaling","title":"Why is Descaling?","text":"<p>While the vast majority of modern anime is released in 1080p (either via web streams or with BluRay releases), many productions are not actually produced in that resolution. Instead, they're animated in some lower resolution and then upscaled to 1080p later in the production pipeline. The original resolution can be a well-known resolution like 540p, 720p, 810p, or 900p, but it can also have more obscure dimensions like 873 or even fractional values like 843.75.</p> <p>When upscaling to 1080p, studios usually use some conventional convolutional scaling algorithm like bilinear or some bicubic kernel. The core idea of rescaling is to invert that upscaling process, and reupscale the footage using some better scaling method like a Waifu2x-based one. An example where this is especially effective is shows that have been upscaled with BicubicSharp (the bicubic kernel with b=0 and c=1), which causes strong aliasing and haloing. But even for other kernels such a rescale can be a noticeable improvement. For example, a Waifu2x-based rescale with Hermite (bicubic with b=0 c=0) as a downscaler can completely remove halos created by upscaling while still being fairly sharp.</p>"},{"location":"filtering/descaling/descaling/#why-upscale-back-to-1080p","title":"Why upscale back to 1080p?","text":"<p>You might wonder why, after descaling to the original resolution, the video should even be upscaled back to 1080p. Shouldn't it be better to just leave it at its original resolution, which is closer to the source? Well, there are two main reasons why this isn't done:</p> <ol> <li>Descaling can only be done to the luma.    The video's chroma is subsampled to 960x540, so it can no longer be descaled.    If the original resolution was, say, 720p, encoding a 720p video would mean storing    its chroma at a resolution 640x360, which would lose even more information.    If one was to instead encode with 444 chroma subsampling, the chroma would    need to be upscaled to 1280x720 which creates its own problems    (since then it'd likely still be upscaled a second time during playback).</li> <li>Shows can rarely be descaled completely.    Often, some elements of shows were added in 1080p after upscaling.    One common example is credits in openings or endings.    It's also possible for different scenes to have different native resolutions,    or for some scenes to not be descaleable at all.</li> </ol>"},{"location":"filtering/descaling/descaling/#descaling-theory","title":"Descaling Theory","text":"<p>As explained above, the difficult part in descaling is finding the parameters used to upscale (in fact, it's easy to make guesses at these parameters, the hard part is determining whether one's guess is correct). These \"parameters\" involve the following:</p> <ul> <li>The original width and height.   These can be integers, but they can also be fractional values when the video was cropped after resizing or   when the resize used a different sample grid model (see below).</li> <li>The sample grid model used when upscaling.   VapourSynth's resize functions and the <code>descale</code> plugin use the common \"match edges\" model   (see the entropymine article for an explanation of these terms).   When a video is upscaled from an integer resolution like 720p using a different model like \"match centers\",   this is equivalent to upscaling with a non-integer source width/height (e.g. a <code>src_height</code> of <code>1080 * 719/1079 \u2248 719.67</code>)   using a \"match edges\" model.   Hence, when descaling such an upscale, these non-integer dimensions would need to be passed to the <code>descale</code> plugin.</li> <li>The kernel used for upscaling. Determining this is often the most difficult part.</li> <li>How the original upscale handled the edges of the frame,   i.e. whether it implicitly padded width values of zero, mirrored the image, or did something else entirely.</li> <li>Any shifts that were applied when upscaling.</li> <li>Any post-processing that was applied after upscaling.   In the vast majority of cases, post-processing after upscaling implies that your source is not descaleable,   but if the post-processing was a somewhat reversible (ideally linear) function like convolutional sharpening, you might still have a chance.</li> <li>What color space the video was upscaled in, i.e. what color space conversions were performed after the upscale.   Note that a nonlinear color space conversion after upscaling can make a fully accurate descale impossible.</li> <li>Whether there are any non-descaleable elements (e.g. credits) in the video or elements that'd otherwise negatively affect the descale   and should be masked out in the descale (e.g. clipped pixels).</li> <li>The order in which the video was upscaled (horizontally, then vertically, or vice-versa).   Since resampling is linear this is usually irrelevant, but it becomes relevant when dealing with clipped values.</li> </ul> <p>Additionally, these parameters (including whether the video is descaleable at all) can vary between scenes or frames, and the parameters for a horizontal descale can be different from the parameters for a vertical descale. In particular, it's possible for a source to be descaleable only along one axis. An example for this is footage that went through the HDCAM format, which subsamples luma to a width of 1440 (from 1920). Such footage will not be horizontally descalable to its original resolution (maybe to 1440 if you're lucky), but can sometimes still be descaled vertically. Whether a source like this is worth descaling is a separate question.</p> <p>The following sections will explain some of these parameters in more detail, as well as how to attempt to determine them.</p>"},{"location":"filtering/descaling/descaling/#source-dimensions","title":"Source Dimensions","text":"<p>The source dimensions are, of course, the essential part of rescaling. Luckily, they're also fairly easy to approximately determine. There exist two methods for determining source resolutions:</p> <ol> <li>The brute-force method, which simply tries descaling the source clip to a large set of resolutions,    scales them back up with the same kernel, and graphs the total error relative to the original image.    It turns out empirically that the resulting graph will show a spike around the \"correct\" resolution even    when the kernel used in the descales is not the correct one.</li> <li>Frequency-based methods, which exploit the behavior of convolutional resampling in the frequency domain.    These include various different methods: On one hand, simply looking at the size of the \"center blob\"    in a two-dimensional frequency plot of an image can already give an indication of whether this image was upscaled,    and (very) approximately by how much.    A slightly more quantitative method is to take the one-dimensional discrete cosine transform of every row (resp. column)    and sum the entry-wise absolute values across all columns (resp. rows).    Due to how resampling behaves in frequency space, the resulting graph will show a spike at the source resolution.    This is the method used by the \"Frequency Analysis\" tab in vs-preview's native resolution plugin.</li> </ol>"},{"location":"filtering/descaling/descaling/#todo","title":"TODO","text":""},{"location":"filtering/descaling/descaling/#evaluating-descales","title":"Evaluating Descales","text":"<p>The above sections explained how to find candidates for descale parameters, but ultimately these were all just heuristics. In the end, every descale needs to be evaluated manually. This is so important that it warrants being repeated even louder:</p> <p>Attention</p> <p>NEVER blindly decide on a descale purely based on graphs and error values. ALWAYS manually verify that your candidate parameters are correct, using the methods explained below.</p> <p>Evaluating a descale entails</p> <ol> <li>Comparing a rescale of the descaled clip with the same parameters to the original clip.    This is the most obvious necessary condition for a descale to be correct.    For the theoretical perfectly descaleable clip, these two should match exactly.    In practice, there can be some slight differences due to quantization and compression noise,    but nonetheless if a same-kernel rescale significantly differs from the original clip,    that always means that the descale cannot be correct.</li> <li>Inspecting the descale itself. An accurate descale will have line-art that looks \"clean\" at least to some degree.    If your descale has higher-order haloing or very fuzzy-looking lines, it's likely not correct even if the rescale is very close to the descale.    Even first-order haloing in a descale is usually an indication of an incorrect descale,    but to our knowledge it's possible in theory (but fairly rare in the wild) for footage to have been sharpened before upscaling.</li> </ol> <p>Comparing the rescale to the source or inspecting a descale is best done around sharp lines. Exactly horizontal or exactly vertical lines are especially good indicators of rescale quality, since around such lines the descale is essentially purely one-dimensional. The same is true for borders when the video was upscaled using zero-padding: here it's crucial to check whether descaling with <code>border_handling=1</code> fixes the borders or not (and whether the rescale matches the original borders).</p> <p>When searching for descales it can and will happen that you simply will not find any set of parameters giving you satisfying results, even if you find one or more parameters that get you closer than any other ones. Once again, in a situation like this, the answer is usually to just give up descaling. There are plenty of other methods to antialias or dehalo that do not carry the same risks as incorrect descales. Not every show has to be descaled.</p>"},{"location":"filtering/descaling/descaling/#example-code-for-evaluating-descales","title":"Example Code for Evaluating Descales","text":"<p>Here's the code snippet that I (arch1t3cht) usually use when evaluating descales. Of course, this is just one possible method and you can just as well use any other system, including some wrapper library like vodesfunc. The important part is that you're able to see the descale, the same-kernel rescale, and the difference clip.</p> <pre><code>from vstools import get_y, depth, set_output\nfrom vskernels import Bilinear, Catrom, BicubicSharp, Lanczos   # import more here if you need\n\n# Load your clip here\n\nclip = depth(get_y(clip), 32)   # take the luma and convert to 32 bits if you haven't already\n\nset_output(clip)\n\nfor kernel in [Bilinear, Catrom, BicubicSharp, Lanczos(3)]:\n    desc = kernel.descale(clip, 1280, 720)\n    resc = kernel.scale(desc, clip.width, clip.height)\n    err = core.std.Expr([clip, resc], \"x y - abs 10 *\")\n\n    name = f\"{kernel.__name__ if hasattr(kernel, '__name__') else kernel.__class__.__name__}\"\n\n    set_output(desc, f\"{name} Descale\")\n    set_output(resc, f\"{name} Rescale\")\n    set_output(err, f\"{name} Diff\")\n</code></pre> <p>Of course, it can be freely adapted to use other kernels, another resolution (possibly a fractional resolution, or possibly multiple resolutions using a second loop), or other additional parameters like border handling. You can comment out one or more of the <code>set_output</code> calls as you wish to only focus on the descales, rescales, or the difference clips. Usually, looking at the difference clips is the easiest way to quickly rule out incorrect parameters. In more ambiguous cases they also make it easier to find areas where the rescales are most inaccurate (e.g. horizontal/vertical lines), which you can then investigate on the descales or rescales. Before deciding that a rescale is accurate, you should inspect all three clips for any possible issues.</p>"},{"location":"filtering/descaling/descaling/#upscaling-with-the-descale-plugin","title":"Upscaling with the Descale Plugin","text":"<p>When descaling with some of the more exotic parameters like custom kernels or blur (in which case you should make sure that you know what you're doing), it becomes harder to rescale with the same parameters since VapourSynth's resize functions support neither of these. In fact this problem arises even for <code>border_handling</code> (unless you want to mess around with manually padding your clip, which can be annoying), but there it's not as much of an issue since that doesn't affect the rest of the frame. To help with this issue, the <code>descale</code> plugin can also upscale clips using the same parameter syntax. For example, a BicubicSharp descale with <code>border_handling=1</code>: <pre><code>desc = clip.descale.Debicubic(1280, 720, b=0, c=1, border_handling=1)\n</code></pre> can be reverted with the exact same keyword parameters with <code>core.descale.Bicubic</code>: <pre><code>resc = desc.descale.Bicubic(clip.width, clip.height, b=0, c=1, border_handling=1)\n</code></pre> Unfortunately, this is not yet supported by vs-kernels. If you want a lightweight similar alternative in the meantime, you can use the following code: <pre><code>class MyKernel():\n    def __init__(self, name, **kwargs):\n        self.name = name\n        self.kwargs = kwargs\n\n    def scale(self, clip, width, height, **kwargs):\n        return getattr(clip.descale, self.name)(width, height, **self.kwargs, **kwargs)\n\n    def descale(self, clip, width, height, **kwargs):\n        return getattr(clip.descale, \"De\" + self.name.lower())(width, height, **self.kwargs, **kwargs)\n\n# Then, define your own kernels like this, which can mostly be used like vs-kernels kernels\nMBilinear = MyKernel(\"Bilinear\")\nMCatrom = MyKernel(\"Bicubic\", b=0, c=0.5)\nMFFmpeg = MyKernel(\"Bicubic\", b=0, c=0.6)\nMLanczos2 = MyKernel(\"Lanczos\", taps=2)\nMLanczos3 = MyKernel(\"Lanczos\", taps=3)\nMLanczos4 = MyKernel(\"Lanczos\", taps=4)\n</code></pre> This method has its own limitations, though, since for example it does not support (de-)scaling in linear light with <code>linear=True</code>.</p>"},{"location":"filtering/descaling/known-resolutions/","title":"A List of Known Native Resolutions","text":"<p>It can be difficult to determine the native resolution of an anime. Many encoders have already done a lot of research themselves, and this page documents the found results.</p> <p>Attention</p> <p>Our knowledge on descaling is ever-improving. Some of these findings may be inaccurate and outdated! Take them with a grain of salt, and do your due diligence. Verify everything yourself before applying a descale to your encode!</p> <p>This table contains the following information:</p> <ol> <li>The show name,    with a link to its AniDB page.    Seasons are separated    into their own rows.    This may extend    to certain specials too.</li> <li>The known native resolutions    and kernels.    This may be a list    of multiple resolutions    with additional information.    in certain cases    If a kernel is unknown,    \"unknown\" will be written down instead.</li> <li>A confidence rating    on whether this show    is descaleable    with current known tools    and techniques.</li> <li>Additional notes    and information.</li> </ol>"},{"location":"filtering/descaling/known-resolutions/#known-native-resolutions","title":"Known Native Resolutions","text":"Anime name Native Resolution(s)/Kernel Descaleable? Notes Blue Archive the Animation 1280x720 (Bilinear)1920x1080* Yes OP and ED are 1920x1080 Gakkou Gurashi! 1280x720 (Lanczos 3-taps)1280x720 (BicubicSharp)* Yes BicubicSharp on frames with overlays Hayate no Gotoku! 1280x720? (Unknown) No HDCAM master Kubikiri Cycle: Aoiro Savant to Zaregotozukai 1279.67...x719.67... (FFmpegBicubic) Yes Match centers model Kuzu no Honkai 1280x720 (Lanczos 3-taps)1280x720 (BicubicSharp)* Yes BicubicSharp on frames with overlays Toaru Majutsu no Index 1280x720 (Bilinear)1440x1080 (Bilinear) Yes* HDCAM master, can be descaled to 720 vertically, but only 1440 horizontally, and requires multiple descales"},{"location":"misc/comparison/","title":"Comparison","text":"<p>Quality comparisons are frequently used within the enthusiast community to compare the video quality offered by different sources/releases. It serves as a great way to distinguish the differences between good and bad sources, and can help you determine which one to download.</p> <p>This guide goes through the process of setting up and effectively utilizing VSPreview, a previewer utility for VapourSynth, to produce useful quality comparisons that will allow you to ascertain which release offers the best visual experience.</p> <p>Warning</p> <p>The goal of this guide is to ensure the video is represented as accurately as possible. Do NOT use this guide as a reference for making encodes where the goal is to make the video look better.</p>"},{"location":"misc/comparison/#vspreview","title":"VSPreview","text":"<p>VSPreview is a previewer application for scripts created in VapourSynth. It features a simple graphical interface to allow you to use VapourSynth's features (and create comparisons) with ease. This should already be installed in your environment if you followed the setup.</p>"},{"location":"misc/comparison/#dependencies","title":"Dependencies","text":"General SetupDolby Vision <p>In order to create comparisons with VSPreview, you will need to install some necessary dependencies.</p> <ul> <li><code>LibP2P</code>, <code>LSMASHSource</code>, <code>Subtext</code> and <code>vs-placebo</code> can be installed using <code>vsrepo</code> from VapourSynth. In your terminal, run the following:</li> </ul> <pre><code>vsrepo.py install libp2p lsmas sub placebo\n</code></pre> <p>Note</p> <p>If <code>vsrepo.py</code> command doesn't work, make sure Windows is set to open <code>.py</code> files with Python. You may also need to add it to the <code>PATHEXT</code> environment variable.</p> <ul> <li><code>awsmfunc</code> can be installed using <code>pip</code>:</li> </ul> <pre><code>python -m pip install git+https://github.com/OpusGang/awsmfunc.git\n</code></pre> <p>If you're working with Dolby Vision (DV) content, you will need to install additional dependencies.</p> <ul> <li><code>libdovi</code> can be installed using <code>vsrepo</code> from VapourSynth. In your terminal, run the following:</li> </ul> <pre><code>vsrepo.py install dovi_library\n</code></pre>"},{"location":"misc/comparison/#usage","title":"Usage","text":"<p>In order to create a comparison, you will need to create a VapourSynth script. This script outlines the parameters and files which VSPreview will use when generating your comparison.</p> <p>Create a file called <code>comp.py</code> and launch it in your favorite text editor.</p>"},{"location":"misc/comparison/#basic-script","title":"Basic Script","text":"<p>Here's a simple <code>comp.py</code> script example that does nothing more than loading the videos and previewing them.</p> <pre><code>from vstools import vs, core, set_output\nfrom awsmfunc import FrameInfo\n\n## File paths: Hold shift and right-click your file in the Windows File Explorer, select copy as path, and paste it here\nclip1 = core.lsmas.LWLibavSource(r\"C:\\Paste\\File\\Path\\Here.mkv\")\nclip2 = core.lsmas.LWLibavSource(r\"C:\\Paste\\File\\Path\\Here.mkv\")\nclip3 = core.lsmas.LWLibavSource(r\"C:\\Paste\\File\\Path\\Here.mkv\")\n\n## Source: Name of the source\nsource1 = \"FirstSourceName\"\nsource2 = \"SecondSourceName\"\nsource3 = \"ThirdSourceName\"\n\n## &lt;Additional comp settings&gt;\n## Place any additional settings you want to use in your comp here\n## &lt;End of additional comp settings&gt;\n\n## Frameinfo: Displays the frame number, type, and group name in the top left corner (no modification required; add/remove lines as needed)\nclip1 = FrameInfo(clip1, source1)\nclip2 = FrameInfo(clip2, source2)\nclip3 = FrameInfo(clip3, source3)\n\n## Output: Comment/uncomment as needed depending on how many clips you're comparing\nset_output(clip1, name=source1)\nset_output(clip2, name=source2)\nset_output(clip3, name=source3)\n</code></pre>"},{"location":"misc/comparison/#common-issues","title":"Common issues","text":"<p>Most of the time, the basic script will not be enough. Different sources may need various adjustments to make a fair comparison, some of which are covered below with small code snippets on how to deal with them.</p>"},{"location":"misc/comparison/#frame-rate","title":"Frame Rate","text":"<p>Sets the source frame rate (fps) based on fractional input (<code>fpsnum</code>/<code>fpsden</code>). For example, <code>fpsnum=24000</code> and <code>fpsden=1001</code> forces the clip frame rate to 23.976 fps. This should be used on sources that have different frame rates that don't automatically stay in sync.</p> <p>Note</p> <p>If a clip stays in sync without changing during scrubbing, you should note that the specific source has dropped or duplicate frames.</p> <pre><code>## Frame rate: Change fps to match other sources (needed for when the previewer is unable to automatically keep them in sync)\nclip1 = core.std.AssumeFPS(clip1, fpsnum=24000, fpsden=1001)\nclip2 = core.std.AssumeFPS(clip2, fpsnum=25000, fpsden=1000)\nclip3 = core.std.AssumeFPS(clip3, fpsnum=24000, fpsden=1000)\n</code></pre>"},{"location":"misc/comparison/#fieldbased","title":"FieldBased","text":"<p>Sets interlaced flagged content that may be progressive as progressive.</p> <pre><code>## FieldBased: Tags the content as progressive (0); used for progressive content tagged as interlaced\nclip1 = core.std.SetFieldBased(clip1, 0)\nclip2 = core.std.SetFieldBased(clip2, 0)\nclip3 = core.std.SetFieldBased(clip3, 0)\n</code></pre>"},{"location":"misc/comparison/#inverse-telecine","title":"Inverse Telecine","text":"<p>Quick inverse telecine filter for converting telecined clips to progressive.</p> <pre><code>## Inverse telecine: Fixes telecined video\nclip1 = core.vivtc.VFM(clip1, 1)\nclip1 = core.vivtc.VDecimate(clip1)\n</code></pre> <p>Note</p> <p>You need <code>vivtc</code> installed for the above snippet to work. You can install it with <code>vsrepo.py install vivtc</code>.</p>"},{"location":"misc/comparison/#cropping","title":"Cropping","text":"<p>Crops the source video by n pixels from the selected side. For example, <code>left=20</code> will remove 20 horizontal pixels starting from the left side. This should be used on sources that use letterboxing or other form of borders.</p> <p>Warning</p> <p>If you are cropping with odd numbers, you will need to convert your clip to 16-bit depth with 4:4:4 chroma subsampling.</p> <pre><code>## Cropping: Removes letterboxing (black bars) [16-bit required for odd numbers]\nclip1 = core.std.Crop(clip1, left=240, right=240, top=0, bottom=0)\nclip2 = core.std.Crop(clip2, left=0, right=0, top=276, bottom=276)\nclip3 = core.std.Crop(clip3, left=0, right=0, top=21, bottom=21)\n</code></pre> <p>Note</p> <p>Make sure to check for variable aspect ratios throughout the file and only crop the smallest border.</p>"},{"location":"misc/comparison/#scaling","title":"Scaling","text":"<p>Downscales or upscales the video. This should be used to match sources that have differing resolutions.</p> <ul> <li>For upscaling (e.g. 720p -&gt; 1080p), use <code>EwaLanczos</code>:</li> </ul> <pre><code>from vskernels import EwaLanczos\n\n## Upscaling: Increases the resolution of clips to match the highest resolution using EwaLanczos (equivalent scaling to mpv's high-quality profile); recommended\nclip1 = EwaLanczos.scale(clip1, 1920, 1080, sigmoid=True)\nclip2 = EwaLanczos.scale(clip2, 1920, 1080, sigmoid=True)\nclip3 = EwaLanczos.scale(clip3, 3840, 2160, sigmoid=True)\n</code></pre> <ul> <li>For downscaling (e.g. 2160p/4K -&gt; 1080p), use <code>Hermite</code>:</li> </ul> <pre><code>from vskernels import Hermite\n\n## Downscaling: Decreases the resolution of clips to match the lowest resolution using Hermite (equivalent scaling to mpv's high-quality profile); not recommended\nclip1 = Hermite.scale(clip1, 1920, 1080, linear=True)\nclip2 = Hermite.scale(clip2, 1920, 1080, linear=True)\nclip3 = Hermite.scale(clip3, 3840, 2160, linear=True)\n</code></pre> <p>Warning</p> <p>Downscaling is generally not recommended. We suggest upscaling your sources to match the highest resolution unless you have a specific reason (e.g. comparing how a higher resolution file would look on a lower resolution display).</p>"},{"location":"misc/comparison/#trimming","title":"Trimming","text":"<p>Removes the first n frames from the source. For example, <code>[24:]</code> will skip the first 24 frames and start the source at frame 25. This should be used on sources that are out of sync.</p> <p>To get the frame difference, find a unique frame (e.g. scene changes) in the correct and incorrect source. Note the frame numbers each one begin at, then set the difference of the two for the incorrect source.</p> <pre><code>## Trimming: Trim frames to match clips (calculate the frame difference and enter the number here)\nclip1 = clip1[0:]\nclip2 = clip2[24:]\nclip3 = clip3[0:]\n</code></pre> <p>Note</p> <p>For more advanced trimming such as chaining, splicing, and looping, see Vapoursynth's docs.</p>"},{"location":"misc/comparison/#depth","title":"Depth","text":"<p>Converts clips to 16-bit depth with 4:4:4 chroma subsampling. Required for filters such as cropping (with odd numbers) or tonemapping.</p> <pre><code>## Depth: Convert clips to 16-bit 4:4:4 [required for cropping with odd numbers or tonemapping]\nclip1 = core.resize.Lanczos(clip1, format=vs.YUV444P16)\nclip2 = core.resize.Lanczos(clip2, format=vs.YUV444P16)\nclip3 = core.resize.Lanczos(clip3, format=vs.YUV444P16)\n</code></pre>"},{"location":"misc/comparison/#tonemapping","title":"Tonemapping","text":"<p>Converts the colorspace of the source (i.e. HDR/DV -&gt; SDR).</p> <ul> <li>For converting HDR (washed out colors) -&gt; SDR, set <code>source_colorspace=ColorSpace.HDR10</code></li> <li>For converting DV (green/purple hue) -&gt; SDR, set <code>source_colorspace=ColorSpace.DOVI</code></li> </ul> <p>Note</p> <p>If you want to tonemap, you will need to change the clip's bit depth to 16-bit (see above).</p> <pre><code>## Additional imports [Paste these at the very top of your script]\nfrom awsmfunc.types.placebo import PlaceboColorSpace as ColorSpace\nfrom awsmfunc.types.placebo import PlaceboTonemapFunction as Tonemap\nfrom awsmfunc.types.placebo import PlaceboGamutMapping as Gamut\nfrom awsmfunc.types.placebo import PlaceboTonemapOpts\n\n## Tonemapping: Converts the dynamic range of the source [16-bit required]\n## Specify the arguments based on your sources; play with different values when comparing against an SDR source to best match it\nclip1args = PlaceboTonemapOpts(source_colorspace=ColorSpace.DOVI, target_colorspace=ColorSpace.SDR, tone_map_function=Tonemap.ST2094_40, gamut_mapping=Gamut.Clip, peak_detect=True, use_dovi=True, contrast_recovery=0.3)\nclip2args = PlaceboTonemapOpts(source_colorspace=ColorSpace.HDR10, target_colorspace=ColorSpace.SDR, tone_map_function=Tonemap.ST2094_40, gamut_mapping=Gamut.Clip, peak_detect=True, use_dovi=False, contrast_recovery=0.3)\nclip3args = PlaceboTonemapOpts(source_colorspace=ColorSpace.HDR10, target_colorspace=ColorSpace.SDR, tone_map_function=Tonemap.Spline, gamut_mapping=Gamut.Darken, peak_detect=True, use_dovi=False, contrast_recovery=0.3, dst_max=120)\n\n## Apply tonemapping\nclip1 = core.placebo.Tonemap(clip1, **clip1args.vsplacebo_dict())\nclip2 = core.placebo.Tonemap(clip2, **clip2args.vsplacebo_dict())\nclip3 = core.placebo.Tonemap(clip3, **clip3args.vsplacebo_dict())\n\n## Retag video to 709 after tonemapping [required]\nclip1 = core.std.SetFrameProps(clip1, _Matrix=vs.MATRIX_BT709, _Transfer=vs.TRANSFER_BT709, _Primaries=vs.PRIMARIES_BT709)\nclip2 = core.std.SetFrameProps(clip2, _Matrix=vs.MATRIX_BT709, _Transfer=vs.TRANSFER_BT709, _Primaries=vs.PRIMARIES_BT709)\nclip3 = core.std.SetFrameProps(clip3, _Matrix=vs.MATRIX_BT709, _Transfer=vs.TRANSFER_BT709, _Primaries=vs.PRIMARIES_BT709)\n</code></pre> <p>Note</p> <p>Refer to the libplacebo and vs-placebo docs to gain a better understanding of what each parameter does.</p>"},{"location":"misc/comparison/#range","title":"Range","text":"<p>Sets the color range of the clip as limited (<code>0</code>) or full (<code>1</code>). This should be used on sources containing incorrect metadata or after tonemapping DV content (set it to limited).</p> <pre><code>## Color range: Marks the clip's range as limited (0) or full (1); DV clips will need to be set to limited (0) after tonemapping\nclip1 = core.resize.Lanczos(clip1, format=vs.YUV444P16, range=0)\nclip2 = core.resize.Lanczos(clip2, format=vs.YUV444P16, range=0)\nclip3 = core.resize.Lanczos(clip3, format=vs.YUV444P16, range=1)\n</code></pre>"},{"location":"misc/comparison/#gamma","title":"Gamma","text":"<p>Adjusts the gamma level of the video. This should only be used to fix the QuickTime gamma bug or similar where one source will appear much brighter than the rest.</p> <pre><code>from vstools import depth\n\n## Gamma: Fixes gamma bug (i.e. one source is significantly brighter than the others) [32-bit required]\n## Convert clips to 32-bit [required for gamma fix]\nclip1 = depth(clip1, 32)\nclip2 = depth(clip2, 32)\nclip3 = depth(clip3, 32)\n## Apply fix\nclip1 = core.std.Levels(clip1, gamma=0.88, planes=0)\nclip2 = core.std.Levels(clip2, gamma=0.88, planes=0)\nclip3 = core.std.Levels(clip3, gamma=0.88, planes=0)\n</code></pre>"},{"location":"misc/comparison/#frameprops","title":"FrameProps","text":"<p>Set the correct frame properties for your sources. This is most commonly used on sources you're upscaling or 4K SDR content. This should be used on sources with incorrect/missing metadata or colors that are off, particularly reds and greens.</p> <pre><code>## FrameProps: Repairs sources with incorrect/missing metadata; typically used for 4K SDR and upscaled/downscaled content (colors will be off, particularly reds, greens, and blues)\n\n# SDR: BD/WEB (720p - 4K)\nclip1 = core.std.SetFrameProps(clip1, _Matrix=vs.MATRIX_BT709, _Transfer=vs.TRANSFER_BT709, _Primaries=vs.PRIMARIES_BT709)\n\n# SDR: PAL DVD\nclip2 = core.std.SetFrameProps(clip2, _Matrix=vs.MATRIX_BT470_BG, _Transfer=vs.TRANSFER_BT470_BG, _Primaries=vs.PRIMARIES_BT470_BG)\n\n# SDR: NTSC DVD\nclip3 = core.std.SetFrameProps(clip3, _Matrix=vs.MATRIX_ST170_M, _Transfer=vs.TRANSFER_BT601, _Primaries=vs.PRIMARIES_ST170_M,)\n\n# HDR/DV\nclip4 = core.std.SetFrameProps(clip4, _Matrix=vs.MATRIX_BT2020_NCL, _Transfer=vs.TRANSFER_BT2020_10, _Primaries=vs.PRIMARIES_BT2020)\n</code></pre>"},{"location":"misc/comparison/#double-range-compression-drc","title":"Double-Range Compression (DRC)","text":"<p>Fixes washed out colors on selected sources.</p> <pre><code>## Fix DRC: Repairs sources with very washed out colors\nclip1 = core.resize.Point(clip1, range_in=0, range=1, dither_type=\"error_diffusion\")\nclip1 = core.std.SetFrameProp(clip1, prop=\"_ColorRange\", intval=1)\n</code></pre>"},{"location":"misc/comparison/#running","title":"Running","text":"<p>To run your comparison script, launch a terminal window in your working directory and run the following:</p> <pre><code>vspreview comp.py\n</code></pre>"},{"location":"misc/comparison/#tips","title":"Tips","text":"<ul> <li>Label your sources clearly.</li> <li>Try to capture a large variety of scenes (e.g. low/high detail, bright/dark, low/high motion).</li> <li>Try to capture frames of the same type.</li> <li>Try to capture <code>P</code> or <code>B</code> type frames when possible. Although it's not always guranteed that your source will have all the picture types (e.g. Crunchyroll WEB-DLs don't have <code>B</code> frames).</li> </ul>"},{"location":"misc/comparison/#basic-keybinds","title":"Basic Keybinds","text":"Key Action <code>Left arrow</code> (&lt;-) Move back n frames (default: n = 1) <code>Right arrow</code> (-&gt;) Move forward n frames (default: n = 1) Number keys Switches to source n (e.g. <code>2</code> switches to <code>clip2</code>) <code>Shift</code> + <code>S</code> Take and save screenshot of the current frame <code>Ctrl</code> + <code>Space</code> Mark current frame number for [semi-automatic] comparisons"},{"location":"misc/comparison/#process","title":"Process","text":"<p>VSPreview offers three methods for creating comparisons:</p> AutomaticSemi-automaticManual <p>Automatic comparisons are created completely without any additional user input. VSPreview will automatically select, capture, and upload frames for you. This is the fastest method for creating comparisons.</p> <p>Semi-automatic comparisons are created with minor user input. VSPreview will automatically capture and upload frame manually marked by the user. This is the recommended method for creating comparisons.</p> <p>Manual comparisons are created completely by the user. VSPreview displays and handles frame capture, while the main actions are performed by the user through the previewer.</p>"},{"location":"misc/comparison/#capturing","title":"Capturing","text":"<ol> <li> <p>In VSPreview, navigate to the bottom bar and toggle the Comp section</p> </li> <li> <p>Fill out these fields:</p> Key Description Collection name The title of your comparison/show Random Number of frames to randomly capture. This should be set to a value higher or equal to 40 frames Picture types The picture type TMDB ID The TMDB ID for the show </li> <li> <p>Hit the Start Upload button under Comp to begin creating your comparison</p> </li> </ol>"},{"location":"misc/comparison/#setup","title":"Setup","text":"<ol> <li> <p>Locate the frame(s) you want to compare</p> <ul> <li>Use <code>Left arrow</code> to go the previous frame and <code>Right arrow</code> to go to the next frame.</li> <li>Use <code>Shift + Left arrow</code> and <code>Shift + Right arrow</code> to navigate <code>N</code> number of frames on either side.</li> </ul> </li> <li> <p>Once you land on a frame you like, mark it with <code>Ctrl</code> + <code>Space</code>.</p> </li> </ol>"},{"location":"misc/comparison/#capturing_1","title":"Capturing","text":"<ol> <li> <p>In VSPreview, navigate to the bottom bar and toggle the Comp section</p> </li> <li> <p>Fill out these fields:</p> Key Description Collection name The title of your comparison/show TMDB ID The TMDB ID for the show </li> <li> <p>Hit the Start Upload button under Comp to begin creating your comparison</p> </li> </ol>"},{"location":"misc/comparison/#capturing_2","title":"Capturing","text":"<ol> <li> <p>Locate the frame(s) you want to compare</p> <ul> <li>Use <code>Left arrow</code> to go the previous frame and <code>Right arrow</code> to go to the next frame.</li> <li>Use <code>Shift + Left arrow</code> and <code>Shift + Right arrow</code> to navigate <code>N</code> number of frames on either side.</li> </ul> </li> <li> <p>Once you land on a frame you like, take its screenshot with <code>Shift</code> + <code>S</code>.</p> </li> <li> <p>Switch to the other sources and take screenshots of their current frame</p> <ul> <li>Press the number keys to change sources (e.g. <code>1</code> for <code>clip1</code>, <code>2</code> for <code>clip2</code>)</li> </ul> </li> <li> <p>Repeat process for the next frames in your comparison</p> </li> </ol> <p>Note</p> <p>If you want to use automatic Slowpoke Pics sorting, make sure your file naming scheme is set to <code>{frame}_{index}_{Name}</code>. By default, all frames are stored within your working directory unless manually changed to a different destination.</p>"},{"location":"misc/comparison/#slowpoke-pics","title":"Slowpoke Pics","text":""},{"location":"misc/comparison/#fetching-account-tokens","title":"Fetching Account Tokens","text":"<p>If you plan on uploading to Slowpoke Pics (slow.pics) under your account, you will need to provide VSPreview with your account token.</p> ChromeFirefox <ul> <li>Visit Slowpoke Pics in your browser and log into your account</li> <li>Open your browser's Developer Tools. You will need to get two values:</li> <li>To get your <code>browserId</code>, go to Application -&gt; Storage -&gt; Local Storage -&gt; <code>https://slow.pics</code>. Copy the key listed there</li> <li>To get your <code>sessionId</code>, go to Network. Refresh the page, then find <code>slow.pics</code>. On the right section open cookies and copy the <code>SLP-SESSION</code> value</li> <li>In VSPreview, go to Settings -&gt; Comp</li> <li>Paste the two values in the boxes provided</li> </ul> <ul> <li>Visit Slowpoke Pics in your browser and log into your account</li> <li>Open your browser's Developer Tools. You will need to get two values:</li> <li>To get your <code>browserId</code>, go to Storage -&gt; Local Storage -&gt; <code>https://slow.pics</code>. Copy the key listed there</li> <li>To get your <code>sessionId</code>, go to Storage -&gt; Cookies -&gt; <code>https://slow.pics</code>. Copy the key listed under <code>SLP-SESSION</code></li> <li>In VSPreview, go to Settings -&gt; Comp</li> <li>Paste the two values in the boxes provided</li> </ul>"}]}